{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "youtube자연어처리0824.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMQ+BSuGOMW1w9S1BCim1LQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choeuneheol/python-practice/blob/master/youtube%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC0824.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 자연어는 일상 생활에서 사용하는 언어\n",
        "- 자연어 처리는 자연어의 의미를 분석 처리하는 일\n",
        "- 텍스트 분류, 감성 분석, 문서 요약, 질의 응답, 음성 인식, 챗봇과 같은 응용\n",
        "\n"
      ],
      "metadata": {
        "id": "svm7sVE28T41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import torch"
      ],
      "metadata": {
        "id": "FDkbCt8lR1UC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 패키지 설치\n",
        "# NLTK(영어 자연어처리 Tool)\n",
        "# konlpy(한글 자연어처리 Tool)\n",
        "# kss(한글 문장 분리기)"
      ],
      "metadata": {
        "id": "iI0Je9SGSuz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install konlpy\n",
        "!pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZCToiutSDW5",
        "outputId": "187ae8ff-e5ba-4ca1-8ab1-fdedf6db66ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kss\n",
            "  Downloading kss-3.5.5.tar.gz (42.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.4 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting emoji==1.2.0\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from kss) (2022.6.2)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.7/dist-packages (from kss) (8.14.0)\n",
            "Building wheels for collected packages: kss\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.5.5-py3-none-any.whl size=42448240 sha256=60784b0e4d2eef14131fa88e7f72b20bc7fbafd273b8d8082ba76a8d18132cd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/92/60/36f882f5fd62ef248bb51e4c9cbe3d114281e41158c2d0caf2\n",
            "Successfully built kss\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.2.0 kss-3.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-1Tokenization\n",
        "# Tokenization : 의미가 있는 단위(Token)로 나누는 작업"
      ],
      "metadata": {
        "id": "vYIphQVGSuUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt') #필요한 데이터 다운로드\n",
        "print()\n",
        "\n",
        "text = \"In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters(such as in a computer program or web page) into a sequence of lexical\\\n",
        "A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner is also a term for the first stage of a lexer. \\\n",
        "A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\"\n",
        "\n",
        "print(\"문단:\", text)\n",
        "print(\"문장\")\n",
        "n=0\n",
        "for sent_i in sent_tokenize(text): #문단을 문장으로 분리한 결과를 하나씩 출력\n",
        "  print(\"{0}번째 문장 : {1}\".format(n, sent_i))\n",
        "  n += 1\n",
        "  print()\n",
        "\n",
        "sentence = \" Hello! this is NLP tutorial.\"\n",
        "print(\"문장:\", sentence)\n",
        "print(\"단어:\", word_tokenize(sentence)) # 문장을 단어로 분리"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJWSX33OTT0H",
        "outputId": "e5930053-490c-4cf4-e41c-7b55ae95eb6b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "문단: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters(such as in a computer program or web page) into a sequence of lexicalA program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n",
            "문장\n",
            "0번째 문장 : In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters(such as in a computer program or web page) into a sequence of lexicalA program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner is also a term for the first stage of a lexer.\n",
            "\n",
            "1번째 문장 : A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n",
            "\n",
            "문장:  Hello! this is NLP tutorial.\n",
            "단어: ['Hello', '!', 'this', 'is', 'NLP', 'tutorial', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-2. KSS(Korean Sentence Splitter)"
      ],
      "metadata": {
        "id": "3OmZHLDITmy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "\n",
        "text = \"안녕하세요. 만나서 반갑습니다.\"\n",
        "print(\"문단:\", text)\n",
        "print(\"문장:\", kss.split_sentences(text)) #한글을 문장으로 분리"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLTjSnKKXHvu",
        "outputId": "db4325ee-a357-4be2-90a7-90ddf4739e96"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문단: 안녕하세요. 만나서 반갑습니다.\n",
            "문장: ['안녕하세요.', '만나서 반갑습니다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-3.pos Tagging\n",
        "### pos Tagging : 각 단어가 어떤 품사인지 구분하는 작업"
      ],
      "metadata": {
        "id": "2fKPtKwIXaXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger') # 필요한 데이터 다운로드\n",
        "print()\n",
        "\n",
        "text = \"I am a boy and you are a girl.\"\n",
        "tokenized_sentence = word_tokenize(text)\n",
        "\n",
        "print(\"문장:\", text)\n",
        "print(\"단어:\", tokenized_sentence)\n",
        "print(\"품사:\", pos_tag(tokenized_sentence))# 각 단어의 품사 태깅\n",
        "\n",
        "# N은명사 VBP는동사 PRP는인칭대명사 CC등이접속사 DT한정사\n",
        "# 구글에 pos tagging을 검색해보면 더 자세히 알수 있다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAQkOkrNXg0l",
        "outputId": "d4a8a5ed-4195-4a8a-f602-c5042e8cd6c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "문장: I am a boy and you are a girl.\n",
            "단어: ['I', 'am', 'a', 'boy', 'and', 'you', 'are', 'a', 'girl', '.']\n",
            "품사: [('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('boy', 'NN'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('girl', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "okt = Okt()\n",
        "kkma = Kkma()\n",
        "kor_text = \"한글 문장도 형태소 분석이 가능할까요?\"\n",
        "print()\n",
        "\n",
        "print(\"문장:\", kor_text)\n",
        "print()\n",
        "\n",
        "print(\"형태소 분석(Okt):\", okt.morphs(kor_text))\n",
        "print(\"품사 태깅(Okt):\", okt.pos(kor_text))\n",
        "print(\"명사 추출(Okt):\", okt.nouns(kor_text))\n",
        "print()\n",
        "\n",
        "print(\"형태소 분석(꼬꼬마):\", kkma.morphs(kor_text))\n",
        "print(\"품사 태깅(꼬꼬마):\", kkma.pos(kor_text))\n",
        "print(\"명사 추출(꼬꼬마):\", kkma.nouns(kor_text))\n",
        "\n",
        "# konlpy 공식 홈페이지\n",
        "# 한나눔, 꼬꼬마, 코모란, Mecab, Okt 등\n",
        "# 링크 : http://konlpy.org/ko/latest/index.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wZBgFopXpP-",
        "outputId": "ff3b4e1c-5c5c-441e-d1f5-7a229265bbec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "문장: 한글 문장도 형태소 분석이 가능할까요?\n",
            "\n",
            "형태소 분석(Okt): ['한글', '문장', '도', '형태소', '분석', '이', '가능할까', '요', '?']\n",
            "품사 태깅(Okt): [('한글', 'Noun'), ('문장', 'Noun'), ('도', 'Josa'), ('형태소', 'Noun'), ('분석', 'Noun'), ('이', 'Josa'), ('가능할까', 'Adjective'), ('요', 'Noun'), ('?', 'Punctuation')]\n",
            "명사 추출(Okt): ['한글', '문장', '형태소', '분석', '요']\n",
            "\n",
            "형태소 분석(꼬꼬마): ['한글', '문장도', '형태소', '분석', '이', '가능', '하', 'ㄹ까요', '?']\n",
            "품사 태깅(꼬꼬마): [('한글', 'NNG'), ('문장도', 'NNG'), ('형태소', 'NNG'), ('분석', 'NNG'), ('이', 'JKS'), ('가능', 'NNG'), ('하', 'XSV'), ('ㄹ까요', 'EFQ'), ('?', 'SF')]\n",
            "명사 추출(꼬꼬마): ['한글', '문장도', '형태소', '분석', '가능']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-4. 표제어 추출 및 어간 추출\n",
        "## 표제어 : 기본 사전형 단어(단어의 뿌리)"
      ],
      "metadata": {
        "id": "R6nqzpvpXp1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "\n",
        "#표제어 추출\n",
        "print(\"단어:\", words)\n",
        "print(\"표제어:\", [lemmatizer.lemmatize(word) for word in words]) #표제어 추출\n",
        "print()\n",
        "\n",
        "# 품사 정보 추가\n",
        "print(\"품사 정보 추가(doing):\", lemmatizer.lemmatize('doing', 'v'))\n",
        "print(\"품사 정보 추가(dies):\", lemmatizer.lemmatize('dies', 'v'))\n",
        "print(\"품사 정보 추가(watched):\", lemmatizer.lemmatize('watched', 'v'))\n",
        "print(\"품사 정보 추가(has):\", lemmatizer.lemmatize('has', 'v'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6ToRbtrYI7d",
        "outputId": "cac1332e-4470-4067-f382-6ea231fb56d8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "표제어: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n",
            "\n",
            "품사 정보 추가(doing): do\n",
            "품사 정보 추가(dies): die\n",
            "품사 정보 추가(watched): watch\n",
            "품사 정보 추가(has): have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 어간 : 단어의 의미를 담고 있는 핵심 부분"
      ],
      "metadata": {
        "id": "98V7Am4eYI3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ", lancaster\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "\n",
        "sentence = \"In linguistic morphology and infomation retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form-generally a written word form.\"\n",
        "tokenized_sentence = word_tokenize(sentence)[:10]\n",
        "\n",
        "#어간 추출\n",
        "print(\"단어:\", tokenized_sentence) # 10개 단어\n",
        "print(\"어간(Porter):\", [porter_stemmer.stem(word) for word in tokenized_sentence]) # PorterStemmer로 어간 추출\n",
        "print(\"djrks(Lancaster):\", [lancaster_stemmer.stem(word) for word in tokenized_sentence]) # LancasterStemmer로 어간 추출"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoS7roFCYI1s",
        "outputId": "4de261ef-69f3-46f9-a7c7-f357adaf7eb5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어: ['In', 'linguistic', 'morphology', 'and', 'infomation', 'retrieval', ',', 'stemming', 'is', 'the']\n",
            "어간(Porter): ['in', 'linguist', 'morpholog', 'and', 'infom', 'retriev', ',', 'stem', 'is', 'the']\n",
            "djrks(Lancaster): ['in', 'lingu', 'morpholog', 'and', 'infom', 'retriev', ',', 'stem', 'is', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-5. 불용어\n",
        "## 불용어 : 큰 의미가 없는 단어"
      ],
      "metadata": {
        "id": "lxn3UekUYIy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print()\n",
        "\n",
        "stop_words_list = stopwords.words(\"english\") #nltk 영어의 불용어 리스트\n",
        "print(\"nltk 불용어 개수:\", len(stop_words_list))\n",
        "print(\"불용어 예시(5개):\", stop_words_list[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JzOUmU4YIwZ",
        "outputId": "0baf95be-58ec-448a-c18e-9c6f540310be"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "nltk 불용어 개수: 179\n",
            "불용어 예시(5개): ['i', 'me', 'my', 'myself', 'we']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"이 문장에서 불용어를 제외하면 무엇이 남을까요?\"\n",
        "stop_words = \"이 에서 를 하면\"\n",
        "\n",
        "#조사, 접속사 제거\n",
        "stop_words = stop_words.split(' ') # 띄어쓰기를 기준으로 문장 분리\n",
        "word_tokens = okt.morphs(example)\n",
        "\n",
        "result = [word for word in word_tokens if not word in stop_words] # 불용어에 해당하는 경우 제거\n",
        "\n",
        "print(\"원래 단어:\", word_tokens)\n",
        "print(\"불용어 제거:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gml_LioUYItz",
        "outputId": "e2b22a41-065c-4bcf-eeac-7cf71b498120"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원래 단어: ['이', '문장', '에서', '불', '용어', '를', '제외', '하면', '무엇', '이', '남을까요', '?']\n",
            "불용어 제거: ['문장', '불', '용어', '제외', '무엇', '남을까요', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-6. 정규표현식\n",
        "\n",
        "## 다양한 텍스트를 하나의 문법 또는 규칙으로 정의"
      ],
      "metadata": {
        "id": "PX9hCj5BYIrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#임의의 문자 1개 \n",
        "r_1 = re.compile(\"a.c\") # a와 c사이에 아무 문자 1개, ex) aac, abc, ...\n",
        "print(\"abc:\", r_1.search(\"abc\")) # a와 c 사이에 문자가 1개 (b) => 0\n",
        "print(\"abbc:\", r_1.search(\"abbc\")) # a와 c 사이에 문자가 2개 (bb) => x\n",
        "print(\"acd\", r_1.search(\"acd\")) # a와 c 사이에 문자가 0개 => x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9i4vLIOYIom",
        "outputId": "d3539fed-88e2-4c05-8bb7-1ccb63f1d2e2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abc: <re.Match object; span=(0, 3), match='abc'>\n",
            "abbc: None\n",
            "acd None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자가 존재하거나 아닌 경우\n",
        "r_2 = re.compile(\"ab?c\")\n",
        "print(\"abc:\", r_2.search(\"abc\")) # a와 c 사이에 b가 존재 (b) => 0\n",
        "print(\"ac:\", r_2.search(\"ac\")) # a와 c 사이에 b가 존재x => 0\n",
        "print(\"ab:\", r_2.search(\"ab\")) # a와 c 라는 패턴이 존재X => x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vFDRElKYImV",
        "outputId": "651111f0-f316-445f-e0d8-e3b7737a6a50"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abc: <re.Match object; span=(0, 3), match='abc'>\n",
            "ac: <re.Match object; span=(0, 2), match='ac'>\n",
            "ab: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#문자가 0개 이상일 경우\n",
        "r_3 = re.compile(\"ab*c\") # ac, abc, abbc, abbbbbc, ...\n",
        "print(\"ac:\", r_3.search(\"ac\")) # a와 c 사이에 b가 존재x => 0\n",
        "print(\"abc:\", r_3.search(\"abc\")) # a와 c 사이에 b가 1개 존재 => 0\n",
        "print(\"abbc:\", r_3.search(\"abbc\")) # a와 c 사이에 b가 2개 존재 => 0\n",
        "print(\"abbdc:\", r_3.search(\"abbdc\")) # a와 c 사이에 d문자가 존재 => x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBopK7WRYIjt",
        "outputId": "517d4af2-0158-4bf6-c72f-c8a3ebe82c18"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ac: <re.Match object; span=(0, 2), match='ac'>\n",
            "abc: <re.Match object; span=(0, 3), match='abc'>\n",
            "abbc: <re.Match object; span=(0, 4), match='abbc'>\n",
            "abbdc: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#문자가 1개 이상일 경우\n",
        "r_4 = re.compile(\"ab+c\") # ac를 제외한 abc(0), abbc, abbbc, ...\n",
        "print(\"ac:\", r_4.search(\"ac\")) # a와 c 사이에 b가 존재x => x\n",
        "print(\"abc:\", r_4.search(\"abc\")) # a와 c 사이에 b가 1개 존재 => 0\n",
        "print(\"abbc:\", r_4.search(\"abbc\")) # a와 c 사이에 b가 2개 존재 => 0\n",
        "print(\"abbdc:\", r_4.search(\"abbdc\")) # a와 c 사이에 d문자가 존재 => x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_A_wjPfYIhb",
        "outputId": "b63dd2b9-4b06-491b-bdfd-420bd2de25cf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ac: None\n",
            "abc: <re.Match object; span=(0, 3), match='abc'>\n",
            "abbc: <re.Match object; span=(0, 4), match='abbc'>\n",
            "abbdc: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#특정 문자열로 시작\n",
        "r_5 = re.compile(\"^a\")\n",
        "print(\"a:\", r_5.search(\"a\")) # a로 시작 => 0\n",
        "print(\"abc:\", r_5.search(\"abc\")) # a로 시작=> 0\n",
        "print(\"ba:\", r_5.search(\"ba\")) # b로 시작=> x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow1HUE14YIfL",
        "outputId": "9051e55d-8650-4edb-ae5c-02e654612bc4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: <re.Match object; span=(0, 1), match='a'>\n",
            "abc: <re.Match object; span=(0, 1), match='a'>\n",
            "ba: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#특정 숫자만큼 반복\n",
        "r_6 = re.compile(\"ab{2}c\") # \"abbc\"\n",
        "print(\"abc:\", r_6.search(\"abc\")) # a와 c 사이에 b가 1번 반복 => x\n",
        "print(\"abbc:\", r_6.search(\"abbc\")) # a와 c 사이에 b가 2번 반복=> 0\n",
        "print(\"abbbc:\", r_6.search(\"abbbc\")) # a와 c 사이에 b가 3번 반복=> x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puB_rnPYYIcx",
        "outputId": "dd8f8199-8660-42c7-e392-5d3963c64bef"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abc: None\n",
            "abbc: <re.Match object; span=(0, 4), match='abbc'>\n",
            "abbbc: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#특정 범위만큼 반복\n",
        "r_7 = re.compile(\"ab{2,3}c\") # abbc, abbbc\n",
        "print(\"abc:\", r_7.search(\"abc\")) # a와 c 사이에 b가 1번 반복 => x\n",
        "print(\"abbc:\", r_7.search(\"abbc\")) # a와 c 사이에 b가 2번 반복=> 0\n",
        "print(\"abbbc:\", r_7.search(\"abbbc\")) # a와 c 사이에 b가 3번 반복=> o\n",
        "print(\"abbbbc:\", r_7.search(\"abbbbc\")) # a와 c 사이에 b가 4번 반복=> x\n",
        "print()\n",
        "\n",
        "r_7_a = re.compile(\"ab{2,}c\") # abbc, abbbc, abbbbc\n",
        "print(\"abc:\", r_7_a.search(\"abc\")) # a와 c 사이에 b가 1번 반복 => x\n",
        "print(\"abbc:\", r_7_a.search(\"abbc\")) # a와 c 사이에 b가 2번 반복=> 0\n",
        "print(\"abbbc:\", r_7_a.search(\"abbbc\")) # a와 c 사이에 b가 3번 반복=> o\n",
        "print(\"abbbbc:\", r_7_a.search(\"abbbbc\")) # a와 c 사이에 b가 4번 반복=> o\n",
        "print()\n",
        "\n",
        "r_7_b = re.compile(\"ab{,2}c\") # abc, abbc\n",
        "print(\"abc:\", r_7_b.search(\"abc\")) # a와 c 사이에 b가 1번 반복 => o\n",
        "print(\"abbc:\", r_7_b.search(\"abbc\")) # a와 c 사이에 b가 2번 반복=> o\n",
        "print(\"abbbc:\", r_7_b.search(\"abbbc\")) # a와 c 사이에 b가 3번 반복=> x\n",
        "print(\"abbbbc:\", r_7_b.search(\"abbbbc\")) # a와 c 사이에 b가 4번 반복=> x\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVWZkKfhYIaX",
        "outputId": "e94ccfff-3eae-4611-f03e-8cc7b63ec928"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abc: None\n",
            "abbc: <re.Match object; span=(0, 4), match='abbc'>\n",
            "abbbc: <re.Match object; span=(0, 5), match='abbbc'>\n",
            "abbbbc: None\n",
            "\n",
            "abc: None\n",
            "abbc: <re.Match object; span=(0, 4), match='abbc'>\n",
            "abbbc: <re.Match object; span=(0, 5), match='abbbc'>\n",
            "abbbbc: <re.Match object; span=(0, 6), match='abbbbc'>\n",
            "\n",
            "abc: <re.Match object; span=(0, 3), match='abc'>\n",
            "abbc: <re.Match object; span=(0, 4), match='abbc'>\n",
            "abbbc: None\n",
            "abbbbc: None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k39Ki7qZYGT0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}