{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choeuneheol/python-practice/blob/master/%EB%AC%B8%EC%A0%9C%ED%95%B4%EA%B2%B0%EC%B1%85DAY11(%EC%9A%B4%EC%A0%84%EC%9E%90%EC%98%88%EC%B8%A1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNbSW-2spyC0",
        "outputId": "e3ba1dce-27aa-46ad-9b23-88639c2415bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\easy1\\anaconda3\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
            "Requirement already satisfied: xgboost in c:\\users\\easy1\\anaconda3\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n",
            "Requirement already satisfied: scipy in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from xgboost) (1.7.3)\n",
            "Requirement already satisfied: lightgbm in c:\\users\\easy1\\anaconda3\\lib\\site-packages (3.3.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from lightgbm) (1.21.5)\n",
            "Requirement already satisfied: wheel in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from lightgbm) (0.37.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from lightgbm) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from lightgbm) (1.1.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install scikit-learn\n",
        "!pip install xgboost\n",
        "!pip install lightgbm\n",
        "\n",
        "train = pd.read_csv('./train.csv')\n",
        "test = pd.read_csv('./test.csv')\n",
        "submission = pd.read_csv('./sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-3ddtIYOcKl"
      },
      "outputs": [],
      "source": [
        "all_data = pd.concat([train, test], ignore_index=True)\n",
        "all_data = all_data.drop('target',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRHRA80eOqlJ",
        "outputId": "d306e0f1-2ab2-4da1-ec67-98faa8f44ae7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['id', 'ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n",
              "       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n",
              "       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n",
              "       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n",
              "       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n",
              "       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n",
              "       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n",
              "       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n",
              "       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n",
              "       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n",
              "       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n",
              "       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n",
              "       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n",
              "       'ps_calc_20_bin'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_features = all_data.columns\n",
        "all_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAyCU6mdO1RU",
        "outputId": "73b311f0-3e15-4925-e4ea-3a60bc956a28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1488028x184 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 20832392 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
        "\n",
        "onehot_encoder = OneHotEncoder()\n",
        "\n",
        "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])\n",
        "\n",
        "encoded_cat_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn6Z80vJQBjR"
      },
      "outputs": [],
      "source": [
        "drop_features = ['ps_ind_14','ps_ind_10_bin','ps_ind_11_bin',\n",
        "                 'ps_ind_12_bin','ps_ind_13_bin','ps_car_14']\n",
        "\n",
        "remaining_features = [feature for feature in all_features\n",
        "                      if('cat' not in feature and\n",
        "                         'calc'not in feature and\n",
        "                         feature not in drop_features)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZNMd1rhQzU1"
      },
      "outputs": [],
      "source": [
        "from scipy import sparse\n",
        "\n",
        "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data[remaining_features]),\n",
        "                               encoded_cat_matrix],\n",
        "                              format='csr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCoyeI9ZRnG4"
      },
      "outputs": [],
      "source": [
        "num_train = len(train)\n",
        "\n",
        "x = all_data_sprs[:num_train]\n",
        "x_test = all_data_sprs[num_train:]\n",
        "\n",
        "y=train['target'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUe-Qf3JR-Xj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def eval_gini(y_true, y_pred):\n",
        "  assert y_true.shape == y_pred.shape\n",
        "\n",
        "  n_samples = y_true.shape[0]\n",
        "  L_mid = np.linspace(1/n_samples, 1, n_samples)\n",
        "\n",
        "  pred_order = y_true[y_pred.argsort()]\n",
        "  L_pred = np.cumsum(pred_order) / np.sum(pred_order)\n",
        "  G_pred = np.sum(L_mid - L_pred)\n",
        "\n",
        "  true_order = y_true[y_true.argsort()]\n",
        "  L_true = np.cumsum(true_order) / np.sum(true_order)\n",
        "  G_true = np.sum(L_mid - L_true)\n",
        "\n",
        "  return G_pred / G_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO7m6o9MTgCJ"
      },
      "outputs": [],
      "source": [
        "def gini(preds, dtrain):\n",
        "  labels = dtrain.get_label()\n",
        "  return 'gini', eval_gini(labels, preds), True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyseO0hgT4ra"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gJ6oedRUrtF"
      },
      "outputs": [],
      "source": [
        "params = {'objective':'binary',\n",
        "          'learning_rate':0.01,\n",
        "          'force_row_wise': True,\n",
        "          'random_state':0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUlZFcCxVmBs"
      },
      "outputs": [],
      "source": [
        "oof_val_preds = np.zeros(x.shape[0])\n",
        "\n",
        "oof_test_preds = np.zeros(x_test.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpGJtHlZv4U2",
        "outputId": "c108eba6-fa49-4017-c50e-4bb4ef0fda90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lightgbm in c:\\users\\easy1\\anaconda3\\lib\\site-packages (3.3.2)Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from lightgbm) (1.1.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from lightgbm) (1.7.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from lightgbm) (1.21.5)\n",
            "Requirement already satisfied: wheel in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from lightgbm) (0.37.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnzhToteWhSQ",
        "outputId": "4b96c333-989b-4518-b42a-31fbe1f7473a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "######################################## 폴드 1 / 폴드 5 ########################################\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
            "[LightGBM] [Info] Total Bins 1350\n",
            "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
            "[LightGBM] [Info] Start training from score -3.274764\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid_0's binary_logloss: 0.15335\tvalid_0's gini: 0.261928\n",
            "[200]\tvalid_0's binary_logloss: 0.152433\tvalid_0's gini: 0.275632\n",
            "[300]\tvalid_0's binary_logloss: 0.152028\tvalid_0's gini: 0.282552\n",
            "[400]\tvalid_0's binary_logloss: 0.151815\tvalid_0's gini: 0.286654\n",
            "[500]\tvalid_0's binary_logloss: 0.151734\tvalid_0's gini: 0.288179\n",
            "[600]\tvalid_0's binary_logloss: 0.151684\tvalid_0's gini: 0.289048\n",
            "[700]\tvalid_0's binary_logloss: 0.151664\tvalid_0's gini: 0.289671\n",
            "[800]\tvalid_0's binary_logloss: 0.151656\tvalid_0's gini: 0.289967\n",
            "[900]\tvalid_0's binary_logloss: 0.151649\tvalid_0's gini: 0.290064\n",
            "Early stopping, best iteration is:\n",
            "[891]\tvalid_0's binary_logloss: 0.151649\tvalid_0's gini: 0.29011\n",
            "폴드 1 지니계수: 0.2901101581027452\n",
            "\n",
            "######################################## 폴드 2 / 폴드 5 ########################################\n",
            "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
            "[LightGBM] [Info] Total Bins 1348\n",
            "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
            "[LightGBM] [Info] Start training from score -3.274764\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid_0's binary_logloss: 0.153496\tvalid_0's gini: 0.249631\n",
            "[200]\tvalid_0's binary_logloss: 0.152709\tvalid_0's gini: 0.260709\n",
            "[300]\tvalid_0's binary_logloss: 0.152392\tvalid_0's gini: 0.26725\n",
            "[400]\tvalid_0's binary_logloss: 0.152243\tvalid_0's gini: 0.271105\n",
            "[500]\tvalid_0's binary_logloss: 0.152184\tvalid_0's gini: 0.272529\n",
            "[600]\tvalid_0's binary_logloss: 0.152162\tvalid_0's gini: 0.272909\n",
            "Early stopping, best iteration is:\n",
            "[567]\tvalid_0's binary_logloss: 0.152158\tvalid_0's gini: 0.273113\n",
            "폴드 2 지니계수: 0.27311339127509615\n",
            "\n",
            "######################################## 폴드 3 / 폴드 5 ########################################\n",
            "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
            "[LightGBM] [Info] Total Bins 1352\n",
            "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
            "[LightGBM] [Info] Start training from score -3.274707\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid_0's binary_logloss: 0.153257\tvalid_0's gini: 0.26157\n",
            "[200]\tvalid_0's binary_logloss: 0.152325\tvalid_0's gini: 0.272212\n",
            "[300]\tvalid_0's binary_logloss: 0.151966\tvalid_0's gini: 0.276811\n",
            "[400]\tvalid_0's binary_logloss: 0.151823\tvalid_0's gini: 0.278215\n",
            "[500]\tvalid_0's binary_logloss: 0.151759\tvalid_0's gini: 0.279724\n",
            "[600]\tvalid_0's binary_logloss: 0.151733\tvalid_0's gini: 0.280294\n",
            "Early stopping, best iteration is:\n",
            "[578]\tvalid_0's binary_logloss: 0.151733\tvalid_0's gini: 0.280352\n",
            "폴드 3 지니계수: 0.28035239226367886\n",
            "\n",
            "######################################## 폴드 4 / 폴드 5 ########################################\n",
            "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
            "[LightGBM] [Info] Total Bins 1351\n",
            "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
            "[LightGBM] [Info] Start training from score -3.274766\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid_0's binary_logloss: 0.1534\tvalid_0's gini: 0.250402\n",
            "[200]\tvalid_0's binary_logloss: 0.152567\tvalid_0's gini: 0.262423\n",
            "[300]\tvalid_0's binary_logloss: 0.152271\tvalid_0's gini: 0.266584\n",
            "[400]\tvalid_0's binary_logloss: 0.152145\tvalid_0's gini: 0.269025\n",
            "[500]\tvalid_0's binary_logloss: 0.152102\tvalid_0's gini: 0.269868\n",
            "[600]\tvalid_0's binary_logloss: 0.152091\tvalid_0's gini: 0.270244\n",
            "[700]\tvalid_0's binary_logloss: 0.152082\tvalid_0's gini: 0.270835\n",
            "[800]\tvalid_0's binary_logloss: 0.152088\tvalid_0's gini: 0.270806\n",
            "Early stopping, best iteration is:\n",
            "[730]\tvalid_0's binary_logloss: 0.152078\tvalid_0's gini: 0.271055\n",
            "폴드 4 지니계수: 0.2710549674411194\n",
            "\n",
            "######################################## 폴드 5 / 폴드 5 ########################################\n",
            "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
            "[LightGBM] [Info] Total Bins 1353\n",
            "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
            "[LightGBM] [Info] Start training from score -3.274766\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid_0's binary_logloss: 0.153492\tvalid_0's gini: 0.261769\n",
            "[200]\tvalid_0's binary_logloss: 0.152645\tvalid_0's gini: 0.273369\n",
            "[300]\tvalid_0's binary_logloss: 0.152305\tvalid_0's gini: 0.279253\n",
            "[400]\tvalid_0's binary_logloss: 0.152102\tvalid_0's gini: 0.284025\n",
            "[500]\tvalid_0's binary_logloss: 0.152026\tvalid_0's gini: 0.286212\n",
            "[600]\tvalid_0's binary_logloss: 0.151994\tvalid_0's gini: 0.287035\n",
            "[700]\tvalid_0's binary_logloss: 0.151992\tvalid_0's gini: 0.287036\n",
            "Early stopping, best iteration is:\n",
            "[627]\tvalid_0's binary_logloss: 0.151989\tvalid_0's gini: 0.287211\n",
            "폴드 5 지니계수: 0.2872110363637672\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "for idx, (train_idx, valid_idx) in enumerate(folds.split(x,y)):\n",
        "\n",
        "  print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}','#'*40)\n",
        "\n",
        "  x_train, y_train = x[train_idx], y[train_idx]\n",
        "  x_valid, y_valid = x[valid_idx], y[valid_idx]\n",
        "\n",
        "  dtrain = lgb.Dataset(x_train, y_train)\n",
        "  dvalid = lgb.Dataset(x_valid, y_valid)\n",
        "\n",
        "  lgb_model = lgb.train(params=params,\n",
        "                        train_set=dtrain,\n",
        "                        num_boost_round=1000,\n",
        "                        valid_sets=dvalid,\n",
        "                        feval = gini,\n",
        "                        early_stopping_rounds=100,\n",
        "                        verbose_eval=100)\n",
        "  \n",
        "  oof_test_preds += lgb_model.predict(x_test)/folds.n_splits\n",
        "\n",
        "  oof_val_preds[valid_idx] += lgb_model.predict(x_valid)\n",
        "\n",
        "  gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
        "\n",
        "  print(f'폴드 {idx+1} 지니계수: {gini_score}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9pyUkidYwz3",
        "outputId": "4b43a88b-d2ae-42e7-ac66-446ec418201f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOF 검증 데이터 지니계수: 0.2802369580498456\n"
          ]
        }
      ],
      "source": [
        "print('OOF 검증 데이터 지니계수:', eval_gini(y, oof_val_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeVWHQjCwy9Y"
      },
      "outputs": [],
      "source": [
        "submission['target'] = oof_test_preds\n",
        "submission.to_csv('submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yva841VxAJf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "submission = pd.read_csv('submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3SeGvZJxb2u"
      },
      "outputs": [],
      "source": [
        "all_data = pd.concat([train, test], ignore_index=True)\n",
        "all_data = all_data.drop('target', axis=1)\n",
        "\n",
        "all_features = all_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0TFtVWzxxYI"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
        "\n",
        "onehot_encoder = OneHotEncoder()\n",
        "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLRkU6IKyUoR"
      },
      "outputs": [],
      "source": [
        "all_data['num_missing'] = (all_data==-1).sum(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGIcSFfGyepT"
      },
      "outputs": [],
      "source": [
        "remaining_features = [feature for feature in all_features\n",
        "                      if ('cat' not in feature and 'calc' not in feature)]\n",
        "remaining_features.append('num_missing')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xParQDPBy4Zw"
      },
      "outputs": [],
      "source": [
        "ind_features = [feature for feature in all_features if 'ind' in feature]\n",
        "\n",
        "is_first_feature = True\n",
        "for ind_feature in ind_features:\n",
        "  if is_first_feature:\n",
        "    all_data['mix_ind'] = all_data[ind_feature].astype(str) + '_'\n",
        "    is_first_feature = False\n",
        "  else:\n",
        "    all_data['mix_ind'] += all_data[ind_feature].astype(str) + '_'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHi0FKVOz0iu",
        "outputId": "392da8b7-710d-4394-9649-837da2cdcbe6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0          2_2_5_1_0_0_1_0_0_0_0_0_0_0_11_0_1_0_\n",
              "1           1_1_7_0_0_0_0_1_0_0_0_0_0_0_3_0_0_1_\n",
              "2          5_4_9_1_0_0_0_1_0_0_0_0_0_0_12_1_0_0_\n",
              "3           0_1_2_0_0_1_0_0_0_0_0_0_0_0_8_1_0_0_\n",
              "4           0_2_0_1_0_1_0_0_0_0_0_0_0_0_9_1_0_0_\n",
              "                           ...                  \n",
              "1488023     0_1_6_0_0_0_1_0_0_0_0_0_0_0_2_0_0_1_\n",
              "1488024    5_3_5_1_0_0_0_1_0_0_0_0_0_0_11_1_0_0_\n",
              "1488025     0_1_5_0_0_1_0_0_0_0_0_0_0_0_5_0_0_1_\n",
              "1488026    6_1_5_1_0_0_0_0_1_0_0_0_0_0_13_1_0_0_\n",
              "1488027    7_1_4_1_0_0_0_0_1_0_0_0_0_0_12_1_0_0_\n",
              "Name: mix_ind, Length: 1488028, dtype: object"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_data['mix_ind']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU5RssvG0b-e",
        "outputId": "dd798b3b-7480-4365-920b-8499888d1895"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              " 1    1079327\n",
              " 2     309747\n",
              " 3      70172\n",
              " 4      28259\n",
              "-1        523\n",
              "Name: ps_ind_02_cat, dtype: int64"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_data['ps_ind_02_cat'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixi5G1FQ0aKV",
        "outputId": "53c8f4a3-9e78-4de3-a00d-d997508b633b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{1: 1079327, 2: 309747, 3: 70172, 4: 28259, -1: 523}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_data['ps_ind_02_cat'].value_counts().to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-pdgfFx0uOp"
      },
      "outputs": [],
      "source": [
        "cat_count_features = []\n",
        "for feature in cat_features+['mix_ind']:\n",
        "  val_counts_dict = all_data[feature].value_counts().to_dict()\n",
        "  all_data[f'{feature}_count'] = all_data[feature].apply(lambda x:\n",
        "                                                         val_counts_dict[x])\n",
        "  cat_count_features.append(f'{feature}_count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPjeeLMz1heN",
        "outputId": "de3401a8-867e-442e-cb83-106dfe9df586"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ps_ind_02_cat_count',\n",
              " 'ps_ind_04_cat_count',\n",
              " 'ps_ind_05_cat_count',\n",
              " 'ps_car_01_cat_count',\n",
              " 'ps_car_02_cat_count',\n",
              " 'ps_car_03_cat_count',\n",
              " 'ps_car_04_cat_count',\n",
              " 'ps_car_05_cat_count',\n",
              " 'ps_car_06_cat_count',\n",
              " 'ps_car_07_cat_count',\n",
              " 'ps_car_08_cat_count',\n",
              " 'ps_car_09_cat_count',\n",
              " 'ps_car_10_cat_count',\n",
              " 'ps_car_11_cat_count',\n",
              " 'mix_ind_count']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cat_count_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQjQ12df1rY1",
        "outputId": "40469df4-c38c-46d5-a6df-b57434386a16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bayesian-optimization in c:\\users\\easy1\\anaconda3\\lib\\site-packages (1.2.0)\n",
            "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.21.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.2.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "from scipy import sparse\n",
        "!pip3 install bayesian-optimization\n",
        "drop_features = ['ps_ind_14','ps_ind_10_bin','ps_ind_11_bin',\n",
        "                 'ps_ind_12_bin','ps_ind_13_bin','ps_car_14']\n",
        "\n",
        "all_data_remaining = all_data[remaining_features+cat_count_features].drop(drop_features, axis=1)\n",
        "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining),\n",
        "                               encoded_cat_matrix],\n",
        "                              format='csr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytDdJNrZ2nYu"
      },
      "outputs": [],
      "source": [
        "num_train = len(train)\n",
        "\n",
        "x = all_data_sprs[:num_train]\n",
        "x_test = all_data_sprs[num_train:]\n",
        "\n",
        "y = train['target'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVKN1nhQ3EdV"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x, y,\n",
        "                                                      test_size=0.2,\n",
        "                                                      random_state=0)\n",
        "\n",
        "bayes_dtrain = lgb.Dataset(x_train, y_train)\n",
        "bayes_dvalid = lgb.Dataset(x_valid, y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TClIcYVq30c7"
      },
      "outputs": [],
      "source": [
        "param_bounds = {'num_leaves' : (30, 40),\n",
        "                'lambda_l1' : (0.7, 0.9),\n",
        "                'lambda_l2' : (0.9, 1),\n",
        "                'feature_fraction' : (0.6, 0.7),\n",
        "                'bagging_fraction' : (0.6, 0.9),\n",
        "                'min_child_samples' : (6, 10),\n",
        "                'min_child_weight' : (10, 40)}\n",
        "\n",
        "fixed_params = {'objective' : 'binary',\n",
        "                'learning_rate' : 0.005,\n",
        "                'bagging_freq' : 1,\n",
        "                'force_row_wise' : True,\n",
        "                'random_state' : 1991}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufMOyjMo46Cb"
      },
      "outputs": [],
      "source": [
        "def eval_function(num_leaves, lambda_l1, lambda_l2, feature_fraction,\n",
        "                  bagging_fraction, min_child_samples, min_child_weight):\n",
        "  '''최적화하려는 평가지표(지니계수) 계산 함수'''\n",
        "\n",
        "  params = {'num_leaves' : int(round(num_leaves)),\n",
        "            'lambda_l1': lambda_l1,\n",
        "            'lambda_l2': lambda_l2,\n",
        "            'feature_fraction': feature_fraction,\n",
        "            'bagging_fraction': bagging_fraction,\n",
        "            'min_child_samples': int(round(min_child_samples)),\n",
        "            'min_child_weight': min_child_weight,\n",
        "            'feature_pre_filter': False}\n",
        "  params.update(fixed_params)\n",
        "\n",
        "  print('하이퍼파라미터:', params)\n",
        "\n",
        "  lgb_model = lgb.train(params=params,\n",
        "                        train_set=bayes_dtrain,\n",
        "                        num_boost_round=2500,\n",
        "                        valid_sets=bayes_dvalid,\n",
        "                        feval=gini,\n",
        "                        early_stopping_rounds=300,\n",
        "                        verbose_eval=False)\n",
        "  preds = lgb_model.predict(x_valid)\n",
        "\n",
        "  gini_score = eval_gini(y_valid, preds)\n",
        "  print(f'지니계수 : {gini_score}\\n')\n",
        "\n",
        "  return gini_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ow3XLbbNG3Y"
      },
      "outputs": [],
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "optimizer = BayesianOptimization(f=eval_function,\n",
        "                                 pbounds=param_bounds,\n",
        "                                 random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pOL7qjpNiSs",
        "outputId": "2aff2f82-7272-4705-aaaf-03d0f6b80ad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.8205526752143287, 'lambda_l2': 0.9544883182996897, 'feature_fraction': 0.6715189366372419, 'bagging_fraction': 0.7646440511781974, 'min_child_samples': 8, 'min_child_weight': 29.376823391999682, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
            "[LightGBM] [Info] Total Bins 1810\n",
            "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 218\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
            "[LightGBM] [Info] Start training from score -3.273091\n",
            "지니계수 : 0.28363010190508786\n",
            "\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.2836  \u001b[0m | \u001b[0m 0.7646  \u001b[0m | \u001b[0m 0.6715  \u001b[0m | \u001b[0m 0.8206  \u001b[0m | \u001b[0m 0.9545  \u001b[0m | \u001b[0m 7.695   \u001b[0m | \u001b[0m 29.38   \u001b[0m | \u001b[0m 34.38   \u001b[0m |\n",
            "하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.867531900234624, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
            "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
            "[LightGBM] [Info] Total Bins 1810\n",
            "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 218\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
            "[LightGBM] [Info] Start training from score -3.273091\n",
            "지니계수 : 0.28307811105958774\n",
            "\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.2831  \u001b[0m | \u001b[0m 0.8675  \u001b[0m | \u001b[0m 0.6964  \u001b[0m | \u001b[0m 0.7767  \u001b[0m | \u001b[0m 0.9792  \u001b[0m | \u001b[0m 8.116   \u001b[0m | \u001b[0m 27.04   \u001b[0m | \u001b[0m 39.26   \u001b[0m |\n",
            "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
            "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
            "[LightGBM] [Info] Total Bins 1810\n",
            "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 218\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
            "[LightGBM] [Info] Start training from score -3.273091\n",
            "지니계수 : 0.2846473606625049\n",
            "\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.2846  \u001b[0m | \u001b[95m 0.6213  \u001b[0m | \u001b[95m 0.6087  \u001b[0m | \u001b[95m 0.704   \u001b[0m | \u001b[95m 0.9833  \u001b[0m | \u001b[95m 9.113   \u001b[0m | \u001b[95m 36.1    \u001b[0m | \u001b[95m 39.79   \u001b[0m |\n",
            "하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.7069445867784718, 'lambda_l2': 0.9993046197458448, 'feature_fraction': 0.6073076882555296, 'bagging_fraction': 0.6, 'min_child_samples': 10, 'min_child_weight': 38.80484823433406, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
            "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Total Bins 1810\n",
            "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 218\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
            "[LightGBM] [Info] Start training from score -3.273091\n",
            "지니계수 : 0.28436117538603967\n",
            "\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.2844  \u001b[0m | \u001b[0m 0.6     \u001b[0m | \u001b[0m 0.6073  \u001b[0m | \u001b[0m 0.7069  \u001b[0m | \u001b[0m 0.9993  \u001b[0m | \u001b[0m 9.503   \u001b[0m | \u001b[0m 38.8    \u001b[0m | \u001b[0m 39.42   \u001b[0m |\n",
            "하이퍼파라미터: {'num_leaves': 36, 'lambda_l1': 0.9, 'lambda_l2': 0.9, 'feature_fraction': 0.6, 'bagging_fraction': 0.9, 'min_child_samples': 6, 'min_child_weight': 36.88499584533189, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
            "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
            "[LightGBM] [Info] Total Bins 1810\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 218\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
            "[LightGBM] [Info] Start training from score -3.273091\n",
            "지니계수 : 0.2837333631329945\n",
            "\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.2837  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.6     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 6.0     \u001b[0m | \u001b[0m 36.88   \u001b[0m | \u001b[0m 36.31   \u001b[0m |\n",
            "하이퍼파라미터: {'num_leaves': 38, 'lambda_l1': 0.894820802468356, 'lambda_l2': 0.9331599201374303, 'feature_fraction': 0.6956119132134094, 'bagging_fraction': 0.6555674497363075, 'min_child_samples': 10, 'min_child_weight': 34.0178291142245, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
            "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
            "[LightGBM] [Info] Total Bins 1810\n",
            "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 218\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
            "[LightGBM] [Info] Start training from score -3.273091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "지니계수 : 0.2834693771806945\n",
            "\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.2835  \u001b[0m | \u001b[0m 0.6556  \u001b[0m | \u001b[0m 0.6956  \u001b[0m | \u001b[0m 0.8948  \u001b[0m | \u001b[0m 0.9332  \u001b[0m | \u001b[0m 9.944   \u001b[0m | \u001b[0m 34.02   \u001b[0m | \u001b[0m 37.82   \u001b[0m |\n",
            "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7, 'lambda_l2': 1.0, 'feature_fraction': 0.6875175159742498, 'bagging_fraction': 0.8673666083940726, 'min_child_samples': 8, 'min_child_weight': 37.27261316578384, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
            "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
            "[LightGBM] [Info] Total Bins 1810\n",
            "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 218\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
            "[LightGBM] [Info] Start training from score -3.273091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "지니계수 : 0.2833493148249361\n",
            "\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.2833  \u001b[0m | \u001b[0m 0.8674  \u001b[0m | \u001b[0m 0.6875  \u001b[0m | \u001b[0m 0.7     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 7.983   \u001b[0m | \u001b[0m 37.27   \u001b[0m | \u001b[0m 40.0    \u001b[0m |\n",
            "하이퍼파라미터: {'num_leaves': 32, 'lambda_l1': 0.829932662510154, 'lambda_l2': 0.9309382272206734, 'feature_fraction': 0.6952194054160209, 'bagging_fraction': 0.6675659367709144, 'min_child_samples': 8, 'min_child_weight': 28.604068940720285, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
            "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
            "[LightGBM] [Info] Total Bins 1810\n",
            "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 218\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
            "[LightGBM] [Info] Start training from score -3.273091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "지니계수 : 0.28406393088341925\n",
            "\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.2841  \u001b[0m | \u001b[0m 0.6676  \u001b[0m | \u001b[0m 0.6952  \u001b[0m | \u001b[0m 0.8299  \u001b[0m | \u001b[0m 0.9309  \u001b[0m | \u001b[0m 7.88    \u001b[0m | \u001b[0m 28.6    \u001b[0m | \u001b[0m 31.94   \u001b[0m |\n",
            "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7455553867388193, 'lambda_l2': 0.9832428375902124, 'feature_fraction': 0.6593794164233446, 'bagging_fraction': 0.7795604634116916, 'min_child_samples': 9, 'min_child_weight': 36.205421876982925, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
            "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
            "[LightGBM] [Info] Total Bins 1810\n",
            "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 218\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
            "[LightGBM] [Info] Start training from score -3.273091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "지니계수 : 0.28332196800720755\n",
            "\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.2833  \u001b[0m | \u001b[0m 0.7796  \u001b[0m | \u001b[0m 0.6594  \u001b[0m | \u001b[0m 0.7456  \u001b[0m | \u001b[0m 0.9832  \u001b[0m | \u001b[0m 9.232   \u001b[0m | \u001b[0m 36.21   \u001b[0m | \u001b[0m 39.69   \u001b[0m |\n",
            "=============================================================================================================\n"
          ]
        }
      ],
      "source": [
        "optimizer.maximize(init_points=3, n_iter=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQMfYRG4REGG",
        "outputId": "173ec4f7-9808-4835-fcee-125b9dd15e73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bagging_fraction': 0.6213108174593661,\n",
              " 'feature_fraction': 0.608712929970154,\n",
              " 'lambda_l1': 0.7040436794880651,\n",
              " 'lambda_l2': 0.9832619845547939,\n",
              " 'min_child_samples': 9.112627003799401,\n",
              " 'min_child_weight': 36.10036444740457,\n",
              " 'num_leaves': 39.78618342232764}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_params=optimizer.max['params']\n",
        "max_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16dlIoUBWjQO"
      },
      "outputs": [],
      "source": [
        "max_params['num_leaves']=int(round(max_params['num_leaves']))\n",
        "max_params['min_child_samples'] = int(round(max_params['min_child_samples']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-QDMET0W6It"
      },
      "outputs": [],
      "source": [
        "max_params.update(fixed_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7B_1XeZXCOR",
        "outputId": "93cf327c-959d-42be-b5ee-e83666835222"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bagging_fraction': 0.6213108174593661,\n",
              " 'feature_fraction': 0.608712929970154,\n",
              " 'lambda_l1': 0.7040436794880651,\n",
              " 'lambda_l2': 0.9832619845547939,\n",
              " 'min_child_samples': 9,\n",
              " 'min_child_weight': 36.10036444740457,\n",
              " 'num_leaves': 40,\n",
              " 'objective': 'binary',\n",
              " 'learning_rate': 0.005,\n",
              " 'bagging_freq': 1,\n",
              " 'force_row_wise': True,\n",
              " 'random_state': 1991}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAMWcsNlXHqL",
        "outputId": "c1842157-7785-4a49-91a5-888db07eeb04",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "######################################## 폴드 1 / 폴드 5 ########################################\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
            "[LightGBM] [Info] Total Bins 1809\n",
            "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
            "[LightGBM] [Info] Start training from score -3.274764\n",
            "Training until validation scores don't improve for 300 rounds\n",
            "[100]\tvalid_0's binary_logloss: 0.154244\tvalid_0's gini: 0.270164\n",
            "[200]\tvalid_0's binary_logloss: 0.153169\tvalid_0's gini: 0.275505\n",
            "[300]\tvalid_0's binary_logloss: 0.152586\tvalid_0's gini: 0.279508\n",
            "[400]\tvalid_0's binary_logloss: 0.152231\tvalid_0's gini: 0.282936\n",
            "[500]\tvalid_0's binary_logloss: 0.152003\tvalid_0's gini: 0.285883\n",
            "[600]\tvalid_0's binary_logloss: 0.151835\tvalid_0's gini: 0.288718\n",
            "[700]\tvalid_0's binary_logloss: 0.15172\tvalid_0's gini: 0.290805\n",
            "[800]\tvalid_0's binary_logloss: 0.151631\tvalid_0's gini: 0.292574\n",
            "[900]\tvalid_0's binary_logloss: 0.151564\tvalid_0's gini: 0.294074\n",
            "[1000]\tvalid_0's binary_logloss: 0.151514\tvalid_0's gini: 0.295092\n",
            "[1100]\tvalid_0's binary_logloss: 0.151472\tvalid_0's gini: 0.296155\n",
            "[1200]\tvalid_0's binary_logloss: 0.151442\tvalid_0's gini: 0.296815\n",
            "[1300]\tvalid_0's binary_logloss: 0.151414\tvalid_0's gini: 0.297429\n",
            "[1400]\tvalid_0's binary_logloss: 0.151393\tvalid_0's gini: 0.297944\n",
            "[1500]\tvalid_0's binary_logloss: 0.151378\tvalid_0's gini: 0.298183\n",
            "[1600]\tvalid_0's binary_logloss: 0.151368\tvalid_0's gini: 0.298429\n",
            "[1700]\tvalid_0's binary_logloss: 0.151356\tvalid_0's gini: 0.298753\n",
            "[1800]\tvalid_0's binary_logloss: 0.151348\tvalid_0's gini: 0.298928\n",
            "[1900]\tvalid_0's binary_logloss: 0.151348\tvalid_0's gini: 0.29892\n",
            "[2000]\tvalid_0's binary_logloss: 0.151349\tvalid_0's gini: 0.298716\n",
            "[2100]\tvalid_0's binary_logloss: 0.15135\tvalid_0's gini: 0.298681\n",
            "Early stopping, best iteration is:\n",
            "[1813]\tvalid_0's binary_logloss: 0.151346\tvalid_0's gini: 0.29899\n",
            "폴드 1 지니계수 : 0.29898999369613055\n",
            "\n",
            "######################################## 폴드 2 / 폴드 5 ########################################\n",
            "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
            "[LightGBM] [Info] Total Bins 1815\n",
            "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
            "[LightGBM] [Info] Start training from score -3.274764\n",
            "Training until validation scores don't improve for 300 rounds\n",
            "[100]\tvalid_0's binary_logloss: 0.154351\tvalid_0's gini: 0.257609\n",
            "[200]\tvalid_0's binary_logloss: 0.153344\tvalid_0's gini: 0.263206\n",
            "[300]\tvalid_0's binary_logloss: 0.152798\tvalid_0's gini: 0.268082\n",
            "[400]\tvalid_0's binary_logloss: 0.152487\tvalid_0's gini: 0.271167\n",
            "[500]\tvalid_0's binary_logloss: 0.152291\tvalid_0's gini: 0.273919\n",
            "[600]\tvalid_0's binary_logloss: 0.152155\tvalid_0's gini: 0.276184\n",
            "[700]\tvalid_0's binary_logloss: 0.15206\tvalid_0's gini: 0.277939\n",
            "[800]\tvalid_0's binary_logloss: 0.151987\tvalid_0's gini: 0.279386\n",
            "[900]\tvalid_0's binary_logloss: 0.151937\tvalid_0's gini: 0.280603\n",
            "[1000]\tvalid_0's binary_logloss: 0.151894\tvalid_0's gini: 0.281572\n",
            "[1100]\tvalid_0's binary_logloss: 0.151866\tvalid_0's gini: 0.282328\n",
            "[1200]\tvalid_0's binary_logloss: 0.151844\tvalid_0's gini: 0.282852\n",
            "[1300]\tvalid_0's binary_logloss: 0.151824\tvalid_0's gini: 0.283409\n",
            "[1400]\tvalid_0's binary_logloss: 0.151806\tvalid_0's gini: 0.283808\n",
            "[1500]\tvalid_0's binary_logloss: 0.151792\tvalid_0's gini: 0.284219\n",
            "[1600]\tvalid_0's binary_logloss: 0.151788\tvalid_0's gini: 0.284346\n",
            "[1700]\tvalid_0's binary_logloss: 0.15178\tvalid_0's gini: 0.284514\n",
            "[1800]\tvalid_0's binary_logloss: 0.151775\tvalid_0's gini: 0.284575\n",
            "[1900]\tvalid_0's binary_logloss: 0.15177\tvalid_0's gini: 0.28477\n",
            "[2000]\tvalid_0's binary_logloss: 0.151767\tvalid_0's gini: 0.285\n",
            "[2100]\tvalid_0's binary_logloss: 0.151774\tvalid_0's gini: 0.284809\n",
            "[2200]\tvalid_0's binary_logloss: 0.151778\tvalid_0's gini: 0.284675\n",
            "Early stopping, best iteration is:\n",
            "[1997]\tvalid_0's binary_logloss: 0.151766\tvalid_0's gini: 0.285\n",
            "폴드 2 지니계수 : 0.28499960024163606\n",
            "\n",
            "######################################## 폴드 3 / 폴드 5 ########################################\n",
            "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
            "[LightGBM] [Info] Total Bins 1813\n",
            "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 218\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
            "[LightGBM] [Info] Start training from score -3.274707\n",
            "Training until validation scores don't improve for 300 rounds\n",
            "[100]\tvalid_0's binary_logloss: 0.154254\tvalid_0's gini: 0.264026\n",
            "[200]\tvalid_0's binary_logloss: 0.153174\tvalid_0's gini: 0.268898\n",
            "[300]\tvalid_0's binary_logloss: 0.152585\tvalid_0's gini: 0.272338\n",
            "[400]\tvalid_0's binary_logloss: 0.152229\tvalid_0's gini: 0.275521\n",
            "[500]\tvalid_0's binary_logloss: 0.15201\tvalid_0's gini: 0.277905\n",
            "[600]\tvalid_0's binary_logloss: 0.151867\tvalid_0's gini: 0.279822\n",
            "[700]\tvalid_0's binary_logloss: 0.151765\tvalid_0's gini: 0.281327\n",
            "[800]\tvalid_0's binary_logloss: 0.151698\tvalid_0's gini: 0.282723\n",
            "[900]\tvalid_0's binary_logloss: 0.151651\tvalid_0's gini: 0.283424\n",
            "[1000]\tvalid_0's binary_logloss: 0.151616\tvalid_0's gini: 0.284118\n",
            "[1100]\tvalid_0's binary_logloss: 0.151587\tvalid_0's gini: 0.284657\n",
            "[1200]\tvalid_0's binary_logloss: 0.151571\tvalid_0's gini: 0.284957\n",
            "[1300]\tvalid_0's binary_logloss: 0.151564\tvalid_0's gini: 0.28499\n",
            "[1400]\tvalid_0's binary_logloss: 0.15156\tvalid_0's gini: 0.285044\n",
            "[1500]\tvalid_0's binary_logloss: 0.151554\tvalid_0's gini: 0.285173\n",
            "[1600]\tvalid_0's binary_logloss: 0.151552\tvalid_0's gini: 0.285223\n",
            "[1700]\tvalid_0's binary_logloss: 0.151556\tvalid_0's gini: 0.285043\n",
            "[1800]\tvalid_0's binary_logloss: 0.151554\tvalid_0's gini: 0.285225\n",
            "Early stopping, best iteration is:\n",
            "[1535]\tvalid_0's binary_logloss: 0.151551\tvalid_0's gini: 0.285292\n",
            "폴드 3 지니계수 : 0.2852920614607706\n",
            "\n",
            "######################################## 폴드 4 / 폴드 5 ########################################\n",
            "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
            "[LightGBM] [Info] Total Bins 1810\n",
            "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
            "[LightGBM] [Info] Start training from score -3.274766\n",
            "Training until validation scores don't improve for 300 rounds\n",
            "[100]\tvalid_0's binary_logloss: 0.154323\tvalid_0's gini: 0.256949\n",
            "[200]\tvalid_0's binary_logloss: 0.153297\tvalid_0's gini: 0.262148\n",
            "[300]\tvalid_0's binary_logloss: 0.152748\tvalid_0's gini: 0.266094\n",
            "[400]\tvalid_0's binary_logloss: 0.152439\tvalid_0's gini: 0.268879\n",
            "[500]\tvalid_0's binary_logloss: 0.152242\tvalid_0's gini: 0.271382\n",
            "[600]\tvalid_0's binary_logloss: 0.152121\tvalid_0's gini: 0.273162\n",
            "[700]\tvalid_0's binary_logloss: 0.15203\tvalid_0's gini: 0.274897\n",
            "[800]\tvalid_0's binary_logloss: 0.151968\tvalid_0's gini: 0.276129\n",
            "[900]\tvalid_0's binary_logloss: 0.151924\tvalid_0's gini: 0.277066\n",
            "[1000]\tvalid_0's binary_logloss: 0.151899\tvalid_0's gini: 0.277571\n",
            "[1100]\tvalid_0's binary_logloss: 0.151878\tvalid_0's gini: 0.278151\n",
            "[1200]\tvalid_0's binary_logloss: 0.151862\tvalid_0's gini: 0.278563\n",
            "[1300]\tvalid_0's binary_logloss: 0.15185\tvalid_0's gini: 0.278966\n",
            "[1400]\tvalid_0's binary_logloss: 0.151842\tvalid_0's gini: 0.27919\n",
            "[1500]\tvalid_0's binary_logloss: 0.151839\tvalid_0's gini: 0.279239\n",
            "[1600]\tvalid_0's binary_logloss: 0.151836\tvalid_0's gini: 0.279297\n",
            "[1700]\tvalid_0's binary_logloss: 0.151829\tvalid_0's gini: 0.279516\n",
            "[1800]\tvalid_0's binary_logloss: 0.15182\tvalid_0's gini: 0.280007\n",
            "[1900]\tvalid_0's binary_logloss: 0.15182\tvalid_0's gini: 0.280113\n",
            "[2000]\tvalid_0's binary_logloss: 0.151821\tvalid_0's gini: 0.280114\n",
            "[2100]\tvalid_0's binary_logloss: 0.151818\tvalid_0's gini: 0.280435\n",
            "Early stopping, best iteration is:\n",
            "[1831]\tvalid_0's binary_logloss: 0.151816\tvalid_0's gini: 0.280157\n",
            "폴드 4 지니계수 : 0.28015748205747726\n",
            "\n",
            "######################################## 폴드 5 / 폴드 5 ########################################\n",
            "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
            "[LightGBM] [Info] Total Bins 1813\n",
            "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 218\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
            "[LightGBM] [Info] Start training from score -3.274766\n",
            "Training until validation scores don't improve for 300 rounds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\tvalid_0's binary_logloss: 0.154403\tvalid_0's gini: 0.267477\n",
            "[200]\tvalid_0's binary_logloss: 0.153386\tvalid_0's gini: 0.272538\n",
            "[300]\tvalid_0's binary_logloss: 0.152834\tvalid_0's gini: 0.276025\n",
            "[400]\tvalid_0's binary_logloss: 0.152495\tvalid_0's gini: 0.279599\n",
            "[500]\tvalid_0's binary_logloss: 0.152273\tvalid_0's gini: 0.28273\n",
            "[600]\tvalid_0's binary_logloss: 0.152133\tvalid_0's gini: 0.284935\n",
            "[700]\tvalid_0's binary_logloss: 0.152022\tvalid_0's gini: 0.287113\n",
            "[800]\tvalid_0's binary_logloss: 0.151943\tvalid_0's gini: 0.288886\n",
            "[900]\tvalid_0's binary_logloss: 0.151893\tvalid_0's gini: 0.289989\n",
            "[1000]\tvalid_0's binary_logloss: 0.151846\tvalid_0's gini: 0.29118\n",
            "[1100]\tvalid_0's binary_logloss: 0.151806\tvalid_0's gini: 0.292204\n",
            "[1200]\tvalid_0's binary_logloss: 0.151783\tvalid_0's gini: 0.292725\n",
            "[1300]\tvalid_0's binary_logloss: 0.15176\tvalid_0's gini: 0.293274\n",
            "[1400]\tvalid_0's binary_logloss: 0.151746\tvalid_0's gini: 0.293713\n",
            "[1500]\tvalid_0's binary_logloss: 0.151735\tvalid_0's gini: 0.294002\n",
            "[1600]\tvalid_0's binary_logloss: 0.151725\tvalid_0's gini: 0.294298\n",
            "[1700]\tvalid_0's binary_logloss: 0.151711\tvalid_0's gini: 0.294751\n",
            "[1800]\tvalid_0's binary_logloss: 0.151713\tvalid_0's gini: 0.294638\n",
            "[1900]\tvalid_0's binary_logloss: 0.151709\tvalid_0's gini: 0.294823\n",
            "[2000]\tvalid_0's binary_logloss: 0.151711\tvalid_0's gini: 0.294817\n",
            "[2100]\tvalid_0's binary_logloss: 0.151721\tvalid_0's gini: 0.294655\n",
            "[2200]\tvalid_0's binary_logloss: 0.151719\tvalid_0's gini: 0.294667\n",
            "Early stopping, best iteration is:\n",
            "[1925]\tvalid_0's binary_logloss: 0.151705\tvalid_0's gini: 0.294975\n",
            "폴드 5 지니계수 : 0.29497487922941584\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)\n",
        "\n",
        "oof_val_preds = np.zeros(x.shape[0])\n",
        "\n",
        "oof_test_preds = np.zeros(x_test.shape[0])\n",
        "\n",
        "for idx, (train_idx, valid_idx) in enumerate(folds.split(x,y)):\n",
        "  print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}','#'*40)\n",
        "\n",
        "  x_train, y_train = x[train_idx], y[train_idx]\n",
        "  x_valid, y_valid = x[valid_idx], y[valid_idx]\n",
        "\n",
        "  dtrain = lgb.Dataset(x_train, y_train)\n",
        "  dvalid = lgb.Dataset(x_valid, y_valid)\n",
        "\n",
        "  lgb_model = lgb.train(params=max_params,\n",
        "                        train_set=dtrain,\n",
        "                        num_boost_round=2500,\n",
        "                        valid_sets=dvalid,\n",
        "                        feval=gini,\n",
        "                        early_stopping_rounds=300,\n",
        "                        verbose_eval=100)\n",
        "  \n",
        "  oof_test_preds += lgb_model.predict(x_test)/folds.n_splits\n",
        "\n",
        "  oof_val_preds[valid_idx] += lgb_model.predict(x_valid)\n",
        "\n",
        "  gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
        "  print(f'폴드 {idx+1} 지니계수 : {gini_score}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Cq2d9jZZtrp",
        "outputId": "6ba4b91d-203b-4294-bb35-7cefccad72f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOF 검증 데이터 지니계수 : 0.288773243692427\n"
          ]
        }
      ],
      "source": [
        "print('OOF 검증 데이터 지니계수 :', eval_gini(y, oof_val_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB53qMscZ5zF"
      },
      "outputs": [],
      "source": [
        "submission['target'] = oof_test_preds\n",
        "submission.to_csv('submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN3jwNCtaHFh"
      },
      "outputs": [],
      "source": [
        "def gini(preds, dtrain):\n",
        "  labels = dtrain.get_label()\n",
        "  return 'gini', eval_gini(labels, preds), True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu6nMJP9aX5n"
      },
      "outputs": [],
      "source": [
        "def gini(preds, dtrain):\n",
        "  labels = dtrain.get_label()\n",
        "  return 'gini', eval_gini(labels,preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xteQfBQpv4U8",
        "outputId": "dd2d924c-9f71-4372-a691-8417c7ef6d23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in c:\\users\\easy1\\anaconda3\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n",
            "Requirement already satisfied: scipy in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from xgboost) (1.7.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plXyMafCamup"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x,y,\n",
        "                                                      test_size=0.2,\n",
        "                                                      random_state=0)\n",
        "\n",
        "bayes_dtrain = xgb.DMatrix(x_train, y_train)\n",
        "bayes_dvalid = xgb.DMatrix(x_valid, y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lyODL49bSKH"
      },
      "outputs": [],
      "source": [
        "param_bounds = {'max_depth':(4, 8),\n",
        "                'subsample':(0.6,0.9),\n",
        "                'colsample_bytree':(0.7, 1.0),\n",
        "                'min_child_weight':(5, 7),\n",
        "                'gamma':(8,11),\n",
        "                'reg_alpha':(7,9),\n",
        "                'reg_lambda':(1.1, 1.5),\n",
        "                'scale_pos_weight':(1.4, 1.6)}\n",
        "\n",
        "fixed_params = {'objective':'binary:logistic',\n",
        "                'learning_rete':0.02,\n",
        "                'random_state':1991}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZlkp_9XcXVs"
      },
      "outputs": [],
      "source": [
        "def eval_function(max_depth, subsample, colsample_bytree, min_child_weight,\n",
        "                  reg_alpha, gamma, reg_lambda, scale_pos_weight):\n",
        "  '''최적화하려는 평가지표(지니계수) 계산 함수'''\n",
        "  params = {'max_depth':int(round(max_depth)),\n",
        "            'subsample':subsample,\n",
        "            'colsample_bytree':colsample_bytree,\n",
        "            'min_child_weight':min_child_weight,\n",
        "            'gamma':gamma,\n",
        "            'reg_alpha':reg_alpha,\n",
        "            'reg_lambda':reg_lambda,\n",
        "            'scale_pos_weight':scale_pos_weight}\n",
        "\n",
        "  params.update(fixed_params)\n",
        "\n",
        "  print('하이퍼파라미터 :', params)\n",
        "\n",
        "  xgb_model = xgb.train(params=params,\n",
        "                        dtrain=bayes_dtrain,\n",
        "                        num_boost_round=2000,\n",
        "                        evals=[(bayes_dvalid, 'bayes_dvalid')],\n",
        "                        maximize=True,\n",
        "                        feval=gini,\n",
        "                        early_stopping_rounds=200,\n",
        "                        verbose_eval=False)\n",
        "  \n",
        "  best_iter = xgb_model.best_iteration\n",
        "\n",
        "  preds = xgb_model.predict(bayes_dvalid,\n",
        "                            iteration_range=(0, best_iter))\n",
        "  \n",
        "  gini_score = eval_gini(y_valid, preds)\n",
        "  print(f'지니계수 : {gini_score}\\n')\n",
        "\n",
        "  return gini_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "TMBF7HJpfSZT",
        "outputId": "b9a66f1a-4e4b-4c64-c6ce-98e01db65b16",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | colsam... |   gamma   | max_depth | min_ch... | reg_alpha | reg_la... | scale_... | subsample |\n",
            "-------------------------------------------------------------------------------------------------------------------------\n",
            "하이퍼파라미터 : {'max_depth': 6, 'subsample': 0.867531900234624, 'colsample_bytree': 0.8646440511781974, 'min_child_weight': 6.0897663659937935, 'gamma': 10.14556809911726, 'reg_alpha': 7.84730959867781, 'reg_lambda': 1.3583576452266626, 'scale_pos_weight': 1.4875174422525386, 'objective': 'binary:logistic', 'learning_rete': 0.02, 'random_state': 1991}\n",
            "[14:36:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\xgboost\\training.py:35: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "지니계수 : 0.272454262578766\n",
            "\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.2725  \u001b[0m | \u001b[0m 0.8646  \u001b[0m | \u001b[0m 10.15   \u001b[0m | \u001b[0m 6.411   \u001b[0m | \u001b[0m 6.09    \u001b[0m | \u001b[0m 7.847   \u001b[0m | \u001b[0m 1.358   \u001b[0m | \u001b[0m 1.488   \u001b[0m | \u001b[0m 0.8675  \u001b[0m |\n",
            "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.6261387899104622, 'colsample_bytree': 0.9890988281503088, 'min_child_weight': 6.0577898395058085, 'gamma': 9.150324556477333, 'reg_alpha': 8.136089122187865, 'reg_lambda': 1.4702386553170643, 'scale_pos_weight': 1.4142072116395774, 'objective': 'binary:logistic', 'learning_rete': 0.02, 'random_state': 1991}\n",
            "[14:37:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "지니계수 : 0.27126092557530607\n",
            "\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.2713  \u001b[0m | \u001b[0m 0.9891  \u001b[0m | \u001b[0m 9.15    \u001b[0m | \u001b[0m 7.167   \u001b[0m | \u001b[0m 6.058   \u001b[0m | \u001b[0m 8.136   \u001b[0m | \u001b[0m 1.47    \u001b[0m | \u001b[0m 1.414   \u001b[0m | \u001b[0m 0.6261  \u001b[0m |\n",
            "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.8341587528859367, 'colsample_bytree': 0.7060655192320977, 'min_child_weight': 6.7400242964936385, 'gamma': 10.497859536643814, 'reg_alpha': 8.957236684465528, 'reg_lambda': 1.4196634256866894, 'scale_pos_weight': 1.4922958724505864, 'objective': 'binary:logistic', 'learning_rete': 0.02, 'random_state': 1991}\n",
            "[14:38:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "지니계수 : 0.2747753737729325\n",
            "\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.2748  \u001b[0m | \u001b[95m 0.7061  \u001b[0m | \u001b[95m 10.5    \u001b[0m | \u001b[95m 7.113   \u001b[0m | \u001b[95m 6.74    \u001b[0m | \u001b[95m 8.957   \u001b[0m | \u001b[95m 1.42    \u001b[0m | \u001b[95m 1.492   \u001b[0m | \u001b[95m 0.8342  \u001b[0m |\n",
            "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.8665097140926193, 'colsample_bytree': 0.7938105131332632, 'min_child_weight': 6.662228501953831, 'gamma': 10.791692477087002, 'reg_alpha': 8.924196666268266, 'reg_lambda': 1.4517319783588594, 'scale_pos_weight': 1.5729649146599027, 'objective': 'binary:logistic', 'learning_rete': 0.02, 'random_state': 1991}\n",
            "[14:38:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\xgboost\\training.py:35: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "지니계수 : 0.27566003503708425\n",
            "\n",
            "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.2757  \u001b[0m | \u001b[95m 0.7938  \u001b[0m | \u001b[95m 10.79   \u001b[0m | \u001b[95m 7.001   \u001b[0m | \u001b[95m 6.662   \u001b[0m | \u001b[95m 8.924   \u001b[0m | \u001b[95m 1.452   \u001b[0m | \u001b[95m 1.573   \u001b[0m | \u001b[95m 0.8665  \u001b[0m |\n",
            "하이퍼파라미터 : {'max_depth': 6, 'subsample': 0.9, 'colsample_bytree': 1.0, 'min_child_weight': 7.0, 'gamma': 11.0, 'reg_alpha': 9.0, 'reg_lambda': 1.5, 'scale_pos_weight': 1.6, 'objective': 'binary:logistic', 'learning_rete': 0.02, 'random_state': 1991}\n",
            "[14:39:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\xgboost\\training.py:35: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "지니계수 : 0.2740800133030967\n",
            "\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.2741  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 11.0    \u001b[0m | \u001b[0m 6.136   \u001b[0m | \u001b[0m 7.0     \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 1.5     \u001b[0m | \u001b[0m 1.6     \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
            "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.9, 'colsample_bytree': 1.0, 'min_child_weight': 6.112939986474294, 'gamma': 11.0, 'reg_alpha': 9.0, 'reg_lambda': 1.1134243398264596, 'scale_pos_weight': 1.6, 'objective': 'binary:logistic', 'learning_rete': 0.02, 'random_state': 1991}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\xgboost\\training.py:35: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[14:40:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "지니계수 : 0.27499874992330325\n",
            "\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.275   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 11.0    \u001b[0m | \u001b[0m 7.426   \u001b[0m | \u001b[0m 6.113   \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 1.113   \u001b[0m | \u001b[0m 1.6     \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
            "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.8717572568885816, 'colsample_bytree': 0.9053643401395246, 'min_child_weight': 6.990795423618443, 'gamma': 10.995830464585534, 'reg_alpha': 8.74111956574225, 'reg_lambda': 1.1500178555011547, 'scale_pos_weight': 1.5446189714626826, 'objective': 'binary:logistic', 'learning_rete': 0.02, 'random_state': 1991}\n",
            "[14:41:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\xgboost\\training.py:35: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "지니계수 : 0.2750306700444254\n",
            "\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.275   \u001b[0m | \u001b[0m 0.9054  \u001b[0m | \u001b[0m 11.0    \u001b[0m | \u001b[0m 7.406   \u001b[0m | \u001b[0m 6.991   \u001b[0m | \u001b[0m 8.741   \u001b[0m | \u001b[0m 1.15    \u001b[0m | \u001b[0m 1.545   \u001b[0m | \u001b[0m 0.8718  \u001b[0m |\n",
            "하이퍼파라미터 : {'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.7, 'min_child_weight': 5.0, 'gamma': 8.0, 'reg_alpha': 7.0, 'reg_lambda': 1.1, 'scale_pos_weight': 1.6, 'objective': 'binary:logistic', 'learning_rete': 0.02, 'random_state': 1991}\n",
            "[14:42:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\xgboost\\training.py:35: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "지니계수 : 0.27727823233016896\n",
            "\n",
            "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.2773  \u001b[0m | \u001b[95m 0.7     \u001b[0m | \u001b[95m 8.0     \u001b[0m | \u001b[95m 4.0     \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 7.0     \u001b[0m | \u001b[95m 1.1     \u001b[0m | \u001b[95m 1.6     \u001b[0m | \u001b[95m 0.9     \u001b[0m |\n",
            "하이퍼파라미터 : {'max_depth': 4, 'subsample': 0.7115165552982349, 'colsample_bytree': 0.7, 'min_child_weight': 5.0, 'gamma': 8.0, 'reg_alpha': 8.01124778269624, 'reg_lambda': 1.1, 'scale_pos_weight': 1.6, 'objective': 'binary:logistic', 'learning_rete': 0.02, 'random_state': 1991}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\xgboost\\training.py:35: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[14:43:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "지니계수 : 0.2772801651675007\n",
            "\n",
            "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.2773  \u001b[0m | \u001b[95m 0.7     \u001b[0m | \u001b[95m 8.0     \u001b[0m | \u001b[95m 4.0     \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 8.011   \u001b[0m | \u001b[95m 1.1     \u001b[0m | \u001b[95m 1.6     \u001b[0m | \u001b[95m 0.7115  \u001b[0m |\n",
            "=========================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "optimizer = BayesianOptimization(f=eval_function,\n",
        "                                 pbounds=param_bounds,\n",
        "                                 random_state=0)\n",
        "\n",
        "optimizer.maximize(init_points=3, n_iter=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDBc9MB9f7P4",
        "outputId": "c0b654a6-8eed-4b56-ca70-d928284df49c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'colsample_bytree': 0.7,\n",
              " 'gamma': 8.0,\n",
              " 'max_depth': 4.0,\n",
              " 'min_child_weight': 5.0,\n",
              " 'reg_alpha': 8.01124778269624,\n",
              " 'reg_lambda': 1.1,\n",
              " 'scale_pos_weight': 1.6,\n",
              " 'subsample': 0.7115165552982349}"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_params = optimizer.max['params']\n",
        "max_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gUQsy_mgDww",
        "outputId": "b2f6794a-5057-4690-fb9f-c61cec301a27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'colsample_bytree': 0.7,\n",
              " 'gamma': 8.0,\n",
              " 'max_depth': 4,\n",
              " 'min_child_weight': 5.0,\n",
              " 'reg_alpha': 8.01124778269624,\n",
              " 'reg_lambda': 1.1,\n",
              " 'scale_pos_weight': 1.6,\n",
              " 'subsample': 0.7115165552982349,\n",
              " 'objective': 'binary:logistic',\n",
              " 'learning_rete': 0.02,\n",
              " 'random_state': 1991}"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
        "max_params.update(fixed_params)\n",
        "max_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK-E8odbv4U9",
        "outputId": "6493926f-a64d-4b3e-c84c-a48f5ce52a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\easy1\\anaconda3\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBslnHGFv4U9",
        "outputId": "b569c1b4-8c5e-4efb-8846-0830a9765d27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\easy1\\anaconda3\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\easy1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGENur2AgXA0",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)\n",
        "\n",
        "oof_val_preds = np.zeros(x.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hvTizbcgxIZ",
        "outputId": "34ef6bb4-de32-40a3-b449-f38dc0c042fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "######################################## 폴드 1 / 폴드 5 ########################################\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\easy1\\anaconda3\\lib\\site-packages\\xgboost\\training.py:35: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[16:11:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[0]\tvalid-logloss:0.48153\tvalid-gini:0.18146\n",
            "[100]\tvalid-logloss:0.15612\tvalid-gini:0.28761\n",
            "[200]\tvalid-logloss:0.15629\tvalid-gini:0.28306\n",
            "[264]\tvalid-logloss:0.15651\tvalid-gini:0.27795\n",
            "폴드 1 지니계수 : 0.29055187743212463\n",
            "\n",
            "######################################## 폴드 2 / 폴드 5 ########################################\n",
            "[16:11:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[0]\tvalid-logloss:0.48150\tvalid-gini:0.15346\n",
            "[100]\tvalid-logloss:0.15655\tvalid-gini:0.27709\n",
            "[200]\tvalid-logloss:0.15673\tvalid-gini:0.27244\n",
            "[287]\tvalid-logloss:0.15701\tvalid-gini:0.26575\n",
            "폴드 2 지니계수 : 0.2784993435893724\n",
            "\n",
            "######################################## 폴드 3 / 폴드 5 ########################################\n",
            "[16:12:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[0]\tvalid-logloss:0.48150\tvalid-gini:0.15700\n",
            "[100]\tvalid-logloss:0.15650\tvalid-gini:0.27167\n",
            "[200]\tvalid-logloss:0.15693\tvalid-gini:0.26573\n",
            "[249]\tvalid-logloss:0.15692\tvalid-gini:0.26514\n",
            "폴드 3 지니계수 : 0.27564097307956165\n",
            "\n",
            "######################################## 폴드 4 / 폴드 5 ########################################\n",
            "[16:12:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[0]\tvalid-logloss:0.48141\tvalid-gini:0.16877\n",
            "[100]\tvalid-logloss:0.15659\tvalid-gini:0.26808\n",
            "[200]\tvalid-logloss:0.15676\tvalid-gini:0.26427\n",
            "[293]\tvalid-logloss:0.15715\tvalid-gini:0.25766\n",
            "폴드 4 지니계수 : 0.26846706000438425\n",
            "\n",
            "######################################## 폴드 5 / 폴드 5 ########################################\n",
            "[16:13:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
            "Parameters: { \"learning_rete\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[0]\tvalid-logloss:0.48155\tvalid-gini:0.17098\n",
            "[100]\tvalid-logloss:0.15637\tvalid-gini:0.28614\n",
            "[200]\tvalid-logloss:0.15653\tvalid-gini:0.28297\n",
            "[293]\tvalid-logloss:0.15665\tvalid-gini:0.27947\n",
            "폴드 5 지니계수 : 0.2863030328592929\n",
            "\n"
          ]
        }
      ],
      "source": [
        "oof_test_preds = np.zeros(x_test.shape[0])\n",
        "\n",
        "for idx, (train_idx, valid_idx) in enumerate(folds.split(x,y)):\n",
        "  print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}','#'*40)\n",
        "\n",
        "  x_train, y_train = x[train_idx], y[train_idx]\n",
        "  x_valid, y_valid = x[valid_idx], y[valid_idx]\n",
        "\n",
        "  dtrain = xgb.DMatrix(x_train, y_train)\n",
        "  dvalid = xgb.DMatrix(x_valid, y_valid)  \n",
        "  dtest = xgb.DMatrix(x_test)\n",
        "\n",
        "  xgb_model = xgb.train(params=max_params,\n",
        "                        dtrain=dtrain,\n",
        "                        num_boost_round=2000,\n",
        "                        evals=[(dvalid, 'valid')],\n",
        "                        maximize=True,\n",
        "                        feval=gini,\n",
        "                        early_stopping_rounds=200,\n",
        "                        verbose_eval=100)\n",
        "  \n",
        "  best_iter = xgb_model.best_iteration\n",
        "\n",
        "  oof_test_preds += xgb_model.predict(dtest,\n",
        "                                     iteration_range=(0, best_iter))/folds.n_splits\n",
        "\n",
        "  oof_val_preds[valid_idx] += xgb_model.predict(dvalid,\n",
        "                                                iteration_range=(0, best_iter))\n",
        "  \n",
        "  gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
        "  print(f'폴드 {idx+1} 지니계수 : {gini_score}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYD-red3ioSE",
        "outputId": "11ba9b13-0593-4398-ccfa-af77b3e5e36f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOF 검증 데이터 지니계수 : 0.2797635481815385\n"
          ]
        }
      ],
      "source": [
        "print('OOF 검증 데이터 지니계수 :', eval_gini(y, oof_val_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg-GSFZYj-pq"
      },
      "outputs": [],
      "source": [
        "submission['target'] = oof_test_preds\n",
        "submission.to_csv('submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfb1crjDkWi2"
      },
      "outputs": [],
      "source": [
        "oof_test_preds = oof_val_preds * 0.5 + oof_val_preds * 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCXMg1q5v4U-",
        "outputId": "b603bba6-bbf4-48fe-d16e-54a0f762562b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.08002885 0.05348538 0.02769019 ... 0.02414416 0.04554787 0.04266113]\n"
          ]
        }
      ],
      "source": [
        "print(oof_test_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xQJEfPkpki3d",
        "outputId": "560d98d5-dd45-4c15-de86-a4887af7f088"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Length of values (595212) does not match length of index (892816)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m submission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m oof_test_preds\n\u001b[0;32m      2\u001b[0m submission\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   3825\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3830\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   3831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3832\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3835\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   3836\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3837\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   3838\u001b[0m     ):\n\u001b[0;32m   3839\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   3840\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4535\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4535\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\common.py:557\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    562\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: Length of values (595212) does not match length of index (892816)"
          ]
        }
      ],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQLZvLsVv4U-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "문제해결책DAY11(운전자예측).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}