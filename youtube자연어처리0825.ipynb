{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "youtube자연어처리0825.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6Yupj0YTLzBw138RZ7Mmm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choeuneheol/python-practice/blob/master/youtube%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC0825.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 자연어는 일상 생활에서 사용하는 언어\n",
        "- 자연어 처리는 자연어의 의미를 분석 처리하는 일\n",
        "- 텍스트 분류, 감성 분석, 문서 요약, 질의 응답, 음성 인식, 챗봇과 같은 응용\n",
        "\n"
      ],
      "metadata": {
        "id": "svm7sVE28T41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import torch"
      ],
      "metadata": {
        "id": "FDkbCt8lR1UC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 패키지 설치\n",
        "# NLTK(영어 자연어처리 Tool)\n",
        "# konlpy(한글 자연어처리 Tool)\n",
        "# kss(한글 문장 분리기)"
      ],
      "metadata": {
        "id": "iI0Je9SGSuz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install konlpy\n",
        "!pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZCToiutSDW5",
        "outputId": "f3e4e355-4241-4167-f8d5-c6ff05137bfb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 67.1 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kss\n",
            "  Downloading kss-3.5.5.tar.gz (42.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.4 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting emoji==1.2.0\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from kss) (2022.6.2)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.7/dist-packages (from kss) (8.14.0)\n",
            "Building wheels for collected packages: kss\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.5.5-py3-none-any.whl size=42448240 sha256=c16bc7e9e938c9404b2858ae7a955a79960e7fa324deab5b396e85b3741d4c1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/92/60/36f882f5fd62ef248bb51e4c9cbe3d114281e41158c2d0caf2\n",
            "Successfully built kss\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.2.0 kss-3.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-1Tokenization\n",
        "# Tokenization : 의미가 있는 단위(Token)로 나누는 작업"
      ],
      "metadata": {
        "id": "vYIphQVGSuUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt') #필요한 데이터 다운로드\n",
        "print()\n",
        "\n",
        "text = \"In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters(such as in a computer program or web page) into a sequence of lexical\\\n",
        "A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner is also a term for the first stage of a lexer. \\\n",
        "A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\"\n",
        "\n",
        "print(\"문단:\", text)\n",
        "print(\"문장\")\n",
        "n=0\n",
        "for sent_i in sent_tokenize(text): #문단을 문장으로 분리한 결과를 하나씩 출력\n",
        "  print(\"{0}번째 문장 : {1}\".format(n, sent_i))\n",
        "  n += 1\n",
        "  print()\n",
        "\n",
        "sentence = \" Hello! this is NLP tutorial.\"\n",
        "print(\"문장:\", sentence)\n",
        "print(\"단어:\", word_tokenize(sentence)) # 문장을 단어로 분리"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJWSX33OTT0H",
        "outputId": "553da2c4-07f5-425f-cef8-bb328dd1f1ac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "문단: In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters(such as in a computer program or web page) into a sequence of lexicalA program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n",
            "문장\n",
            "0번째 문장 : In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters(such as in a computer program or web page) into a sequence of lexicalA program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner is also a term for the first stage of a lexer.\n",
            "\n",
            "1번째 문장 : A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\n",
            "\n",
            "문장:  Hello! this is NLP tutorial.\n",
            "단어: ['Hello', '!', 'this', 'is', 'NLP', 'tutorial', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-2. KSS(Korean Sentence Splitter)"
      ],
      "metadata": {
        "id": "3OmZHLDITmy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "\n",
        "text = \"안녕하세요. 만나서 반갑습니다.\"\n",
        "print(\"문단:\", text)\n",
        "print(\"문장:\", kss.split_sentences(text)) #한글을 문장으로 분리"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLTjSnKKXHvu",
        "outputId": "cf9fa730-a59a-4444-c405-7236910e453f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문단: 안녕하세요. 만나서 반갑습니다.\n",
            "문장: ['안녕하세요.', '만나서 반갑습니다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-3.pos Tagging\n",
        "### pos Tagging : 각 단어가 어떤 품사인지 구분하는 작업"
      ],
      "metadata": {
        "id": "2fKPtKwIXaXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger') # 필요한 데이터 다운로드\n",
        "print()\n",
        "\n",
        "text = \"I am a boy and you are a girl.\"\n",
        "tokenized_sentence = word_tokenize(text)\n",
        "\n",
        "print(\"문장:\", text)\n",
        "print(\"단어:\", tokenized_sentence)\n",
        "print(\"품사:\", pos_tag(tokenized_sentence))# 각 단어의 품사 태깅\n",
        "\n",
        "# N은명사 VBP는동사 PRP는인칭대명사 CC등이접속사 DT한정사\n",
        "# 구글에 pos tagging을 검색해보면 더 자세히 알수 있다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAQkOkrNXg0l",
        "outputId": "a9fae99e-0701-44f8-f3d7-c58629bcd2dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "문장: I am a boy and you are a girl.\n",
            "단어: ['I', 'am', 'a', 'boy', 'and', 'you', 'are', 'a', 'girl', '.']\n",
            "품사: [('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('boy', 'NN'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('girl', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "okt = Okt()\n",
        "kkma = Kkma()\n",
        "kor_text = \"한글 문장도 형태소 분석이 가능할까요?\"\n",
        "print()\n",
        "\n",
        "print(\"문장:\", kor_text)\n",
        "print()\n",
        "\n",
        "print(\"형태소 분석(Okt):\", okt.morphs(kor_text))\n",
        "print(\"품사 태깅(Okt):\", okt.pos(kor_text))\n",
        "print(\"명사 추출(Okt):\", okt.nouns(kor_text))\n",
        "print()\n",
        "\n",
        "print(\"형태소 분석(꼬꼬마):\", kkma.morphs(kor_text))\n",
        "print(\"품사 태깅(꼬꼬마):\", kkma.pos(kor_text))\n",
        "print(\"명사 추출(꼬꼬마):\", kkma.nouns(kor_text))\n",
        "\n",
        "# konlpy 공식 홈페이지\n",
        "# 한나눔, 꼬꼬마, 코모란, Mecab, Okt 등\n",
        "# 링크 : http://konlpy.org/ko/latest/index.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wZBgFopXpP-",
        "outputId": "bbe3e043-b68f-4fd5-8f3a-077d7a42bc99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "문장: 한글 문장도 형태소 분석이 가능할까요?\n",
            "\n",
            "형태소 분석(Okt): ['한글', '문장', '도', '형태소', '분석', '이', '가능할까', '요', '?']\n",
            "품사 태깅(Okt): [('한글', 'Noun'), ('문장', 'Noun'), ('도', 'Josa'), ('형태소', 'Noun'), ('분석', 'Noun'), ('이', 'Josa'), ('가능할까', 'Adjective'), ('요', 'Noun'), ('?', 'Punctuation')]\n",
            "명사 추출(Okt): ['한글', '문장', '형태소', '분석', '요']\n",
            "\n",
            "형태소 분석(꼬꼬마): ['한글', '문장도', '형태소', '분석', '이', '가능', '하', 'ㄹ까요', '?']\n",
            "품사 태깅(꼬꼬마): [('한글', 'NNG'), ('문장도', 'NNG'), ('형태소', 'NNG'), ('분석', 'NNG'), ('이', 'JKS'), ('가능', 'NNG'), ('하', 'XSV'), ('ㄹ까요', 'EFQ'), ('?', 'SF')]\n",
            "명사 추출(꼬꼬마): ['한글', '문장도', '형태소', '분석', '가능']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-4. 표제어 추출 및 어간 추출\n",
        "## 표제어 : 기본 사전형 단어(단어의 뿌리)"
      ],
      "metadata": {
        "id": "R6nqzpvpXp1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "\n",
        "#표제어 추출\n",
        "print(\"단어:\", words)\n",
        "print(\"표제어:\", [lemmatizer.lemmatize(word) for word in words]) #표제어 추출\n",
        "print()\n",
        "\n",
        "# 품사 정보 추가\n",
        "print(\"품사 정보 추가(doing):\", lemmatizer.lemmatize('doing', 'v'))\n",
        "print(\"품사 정보 추가(dies):\", lemmatizer.lemmatize('dies', 'v'))\n",
        "print(\"품사 정보 추가(watched):\", lemmatizer.lemmatize('watched', 'v'))\n",
        "print(\"품사 정보 추가(has):\", lemmatizer.lemmatize('has', 'v'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6ToRbtrYI7d",
        "outputId": "03a14314-c1aa-4625-faa6-51db93163995"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "표제어: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n",
            "\n",
            "품사 정보 추가(doing): do\n",
            "품사 정보 추가(dies): die\n",
            "품사 정보 추가(watched): watch\n",
            "품사 정보 추가(has): have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 어간 : 단어의 의미를 담고 있는 핵심 부분"
      ],
      "metadata": {
        "id": "98V7Am4eYI3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ", lancaster\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "\n",
        "sentence = \"In linguistic morphology and infomation retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form-generally a written word form.\"\n",
        "tokenized_sentence = word_tokenize(sentence)[:10]\n",
        "\n",
        "#어간 추출\n",
        "print(\"단어:\", tokenized_sentence) # 10개 단어\n",
        "print(\"어간(Porter):\", [porter_stemmer.stem(word) for word in tokenized_sentence]) # PorterStemmer로 어간 추출\n",
        "print(\"djrks(Lancaster):\", [lancaster_stemmer.stem(word) for word in tokenized_sentence]) # LancasterStemmer로 어간 추출"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoS7roFCYI1s",
        "outputId": "66599651-5ff6-47a6-9be9-c1f95c426bfa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어: ['In', 'linguistic', 'morphology', 'and', 'infomation', 'retrieval', ',', 'stemming', 'is', 'the']\n",
            "어간(Porter): ['in', 'linguist', 'morpholog', 'and', 'infom', 'retriev', ',', 'stem', 'is', 'the']\n",
            "djrks(Lancaster): ['in', 'lingu', 'morpholog', 'and', 'infom', 'retriev', ',', 'stem', 'is', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-5. 불용어\n",
        "## 불용어 : 큰 의미가 없는 단어"
      ],
      "metadata": {
        "id": "lxn3UekUYIy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print()\n",
        "\n",
        "stop_words_list = stopwords.words(\"english\") #nltk 영어의 불용어 리스트\n",
        "print(\"nltk 불용어 개수:\", len(stop_words_list))\n",
        "print(\"불용어 예시(5개):\", stop_words_list[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JzOUmU4YIwZ",
        "outputId": "99f97132-8369-4e7f-d6fd-e042917ac040"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "nltk 불용어 개수: 179\n",
            "불용어 예시(5개): ['i', 'me', 'my', 'myself', 'we']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"이 문장에서 불용어를 제외하면 무엇이 남을까요?\"\n",
        "stop_words = \"이 에서 를 하면\"\n",
        "\n",
        "#조사, 접속사 제거\n",
        "stop_words = stop_words.split(' ') # 띄어쓰기를 기준으로 문장 분리\n",
        "word_tokens = okt.morphs(example)\n",
        "\n",
        "result = [word for word in word_tokens if not word in stop_words] # 불용어에 해당하는 경우 제거\n",
        "\n",
        "print(\"원래 단어:\", word_tokens)\n",
        "print(\"불용어 제거:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gml_LioUYItz",
        "outputId": "02bd8211-9ae6-4b23-89c6-18c49662080f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원래 단어: ['이', '문장', '에서', '불', '용어', '를', '제외', '하면', '무엇', '이', '남을까요', '?']\n",
            "불용어 제거: ['문장', '불', '용어', '제외', '무엇', '남을까요', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-6. 정규표현식\n",
        "\n",
        "## 다양한 텍스트를 하나의 문법 또는 규칙으로 정의"
      ],
      "metadata": {
        "id": "PX9hCj5BYIrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#임의의 문자 1개 \n",
        "r_1 = re.compile(\"a.c\") # a와 c사이에 아무 문자 1개, ex) aac, abc, ...\n",
        "print(\"abc:\", r_1.search(\"abc\")) # a와 c 사이에 문자가 1개 (b) => 0\n",
        "print(\"abbc:\", r_1.search(\"abbc\")) # a와 c 사이에 문자가 2개 (bb) => x\n",
        "print(\"acd\", r_1.search(\"acd\")) # a와 c 사이에 문자가 0개 => x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9i4vLIOYIom",
        "outputId": "a85828c5-c4da-4e72-e209-abf2294b2e9d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abc: <re.Match object; span=(0, 3), match='abc'>\n",
            "abbc: None\n",
            "acd None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자가 존재하거나 아닌 경우\n",
        "r_2 = re.compile(\"ab?c\")\n",
        "print(\"abc:\", r_2.search(\"abc\")) # a와 c 사이에 b가 존재 (b) => 0\n",
        "print(\"ac:\", r_2.search(\"ac\")) # a와 c 사이에 b가 존재x => 0\n",
        "print(\"ab:\", r_2.search(\"ab\")) # a와 c 라는 패턴이 존재X => x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vFDRElKYImV",
        "outputId": "05284ce0-968b-46f1-c542-31da313572c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abc: <re.Match object; span=(0, 3), match='abc'>\n",
            "ac: <re.Match object; span=(0, 2), match='ac'>\n",
            "ab: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#문자가 0개 이상일 경우\n",
        "r_3 = re.compile(\"ab*c\") # ac, abc, abbc, abbbbbc, ...\n",
        "print(\"ac:\", r_3.search(\"ac\")) # a와 c 사이에 b가 존재x => 0\n",
        "print(\"abc:\", r_3.search(\"abc\")) # a와 c 사이에 b가 1개 존재 => 0\n",
        "print(\"abbc:\", r_3.search(\"abbc\")) # a와 c 사이에 b가 2개 존재 => 0\n",
        "print(\"abbdc:\", r_3.search(\"abbdc\")) # a와 c 사이에 d문자가 존재 => x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBopK7WRYIjt",
        "outputId": "882a3748-201f-4eb4-c446-a1c2a53fb258"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ac: <re.Match object; span=(0, 2), match='ac'>\n",
            "abc: <re.Match object; span=(0, 3), match='abc'>\n",
            "abbc: <re.Match object; span=(0, 4), match='abbc'>\n",
            "abbdc: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#문자가 1개 이상일 경우\n",
        "r_4 = re.compile(\"ab+c\") # ac를 제외한 abc(0), abbc, abbbc, ...\n",
        "print(\"ac:\", r_4.search(\"ac\")) # a와 c 사이에 b가 존재x => x\n",
        "print(\"abc:\", r_4.search(\"abc\")) # a와 c 사이에 b가 1개 존재 => 0\n",
        "print(\"abbc:\", r_4.search(\"abbc\")) # a와 c 사이에 b가 2개 존재 => 0\n",
        "print(\"abbdc:\", r_4.search(\"abbdc\")) # a와 c 사이에 d문자가 존재 => x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_A_wjPfYIhb",
        "outputId": "e808f8e0-38c1-4f5f-edff-4a666e5f1ecc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ac: None\n",
            "abc: <re.Match object; span=(0, 3), match='abc'>\n",
            "abbc: <re.Match object; span=(0, 4), match='abbc'>\n",
            "abbdc: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#특정 문자열로 시작\n",
        "r_5 = re.compile(\"^a\")\n",
        "print(\"a:\", r_5.search(\"a\")) # a로 시작 => 0\n",
        "print(\"abc:\", r_5.search(\"abc\")) # a로 시작=> 0\n",
        "print(\"ba:\", r_5.search(\"ba\")) # b로 시작=> x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow1HUE14YIfL",
        "outputId": "2b282f9f-263b-47ff-f640-6b2371d012f0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: <re.Match object; span=(0, 1), match='a'>\n",
            "abc: <re.Match object; span=(0, 1), match='a'>\n",
            "ba: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#특정 숫자만큼 반복\n",
        "r_6 = re.compile(\"ab{2}c\") # \"abbc\"\n",
        "print(\"abc:\", r_6.search(\"abc\")) # a와 c 사이에 b가 1번 반복 => x\n",
        "print(\"abbc:\", r_6.search(\"abbc\")) # a와 c 사이에 b가 2번 반복=> 0\n",
        "print(\"abbbc:\", r_6.search(\"abbbc\")) # a와 c 사이에 b가 3번 반복=> x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puB_rnPYYIcx",
        "outputId": "d3ab8240-bd10-4b03-c4c6-0bccd02c5c4f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abc: None\n",
            "abbc: <re.Match object; span=(0, 4), match='abbc'>\n",
            "abbbc: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#특정 범위만큼 반복\n",
        "r_7 = re.compile(\"ab{2,3}c\") # abbc, abbbc\n",
        "print(\"abc:\", r_7.search(\"abc\")) # a와 c 사이에 b가 1번 반복 => x\n",
        "print(\"abbc:\", r_7.search(\"abbc\")) # a와 c 사이에 b가 2번 반복=> 0\n",
        "print(\"abbbc:\", r_7.search(\"abbbc\")) # a와 c 사이에 b가 3번 반복=> o\n",
        "print(\"abbbbc:\", r_7.search(\"abbbbc\")) # a와 c 사이에 b가 4번 반복=> x\n",
        "print()\n",
        "\n",
        "r_7_a = re.compile(\"ab{2,}c\") # abbc, abbbc, abbbbc\n",
        "print(\"abc:\", r_7_a.search(\"abc\")) # a와 c 사이에 b가 1번 반복 => x\n",
        "print(\"abbc:\", r_7_a.search(\"abbc\")) # a와 c 사이에 b가 2번 반복=> 0\n",
        "print(\"abbbc:\", r_7_a.search(\"abbbc\")) # a와 c 사이에 b가 3번 반복=> o\n",
        "print(\"abbbbc:\", r_7_a.search(\"abbbbc\")) # a와 c 사이에 b가 4번 반복=> o\n",
        "print()\n",
        "\n",
        "r_7_b = re.compile(\"ab{,2}c\") # abc, abbc\n",
        "print(\"abc:\", r_7_b.search(\"abc\")) # a와 c 사이에 b가 1번 반복 => o\n",
        "print(\"abbc:\", r_7_b.search(\"abbc\")) # a와 c 사이에 b가 2번 반복=> o\n",
        "print(\"abbbc:\", r_7_b.search(\"abbbc\")) # a와 c 사이에 b가 3번 반복=> x\n",
        "print(\"abbbbc:\", r_7_b.search(\"abbbbc\")) # a와 c 사이에 b가 4번 반복=> x\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVWZkKfhYIaX",
        "outputId": "5bc28089-6ef4-4bd5-aecb-971dab3fe04a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abc: None\n",
            "abbc: <re.Match object; span=(0, 4), match='abbc'>\n",
            "abbbc: <re.Match object; span=(0, 5), match='abbbc'>\n",
            "abbbbc: None\n",
            "\n",
            "abc: None\n",
            "abbc: <re.Match object; span=(0, 4), match='abbc'>\n",
            "abbbc: <re.Match object; span=(0, 5), match='abbbc'>\n",
            "abbbbc: <re.Match object; span=(0, 6), match='abbbbc'>\n",
            "\n",
            "abc: <re.Match object; span=(0, 3), match='abc'>\n",
            "abbc: <re.Match object; span=(0, 4), match='abbc'>\n",
            "abbbc: None\n",
            "abbbbc: None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 특정 문자열 중 하나\n",
        "r_8 = re.compile(\"[abc]\") # a or b or c\n",
        "print(\"ade:\", r_8.search(\"ade\")) # a, b, c 중 a가 존재 => o\n",
        "print(\"deb:\", r_8.search(\"deb\")) # a, b, c 중 b가 존재 => o\n",
        "print(\"dce:\", r_8.search(\"dce\")) # a, b, c 중 가 존c재 => o\n",
        "print(\"def:\", r_8.search(\"def\")) # a, b, c 중 존재 => x\n",
        "print()\n",
        "\n",
        "r_8_a = re.compile(\"[a-z]\") # [abcdefg ... xyz]\n",
        "print(\"a:\", r_8_a.search(\"a\")) # a~z 중 a가 존재 => o\n",
        "print(\"y:\", r_8_a.search(\"y\")) # a~z 중 y가 존재 => o\n",
        "print(\"#:\", r_8_a.search(\"#\")) # a~z 중 존재 => x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k39Ki7qZYGT0",
        "outputId": "40edf2bc-2675-4c04-ead9-ea8089a8eea3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ade: <re.Match object; span=(0, 1), match='a'>\n",
            "deb: <re.Match object; span=(2, 3), match='b'>\n",
            "dce: <re.Match object; span=(1, 2), match='c'>\n",
            "def: None\n",
            "\n",
            "a: <re.Match object; span=(0, 1), match='a'>\n",
            "y: <re.Match object; span=(0, 1), match='y'>\n",
            "#: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 특정 문자열들을 제외\n",
        "r_9 = re.compile(\"[^abc]\") # \n",
        "print(\"abc:\", r_9.search(\"abc\")) # a, b, c가 아닌 것들 중 존재x => x\n",
        "print(\"abd:\", r_9.search(\"abd\")) # a, b, c가 아닌 것들 중 d가 존재 => o\n",
        "print()\n",
        "\n",
        "r_9_a = re.compile(\"[^a-z]\") # \n",
        "print(\"a:\", r_9_a.search(\"a\")) # a~z가 아닌 것들 중 존재x => x\n",
        "print(\"y:\", r_9_a.search(\"y\")) # a~z가 아닌 것들 중 존재x => x\n",
        "print(\"#:\", r_9_a.search(\"#\")) # a~z가 아닌 것들 중 #존재x => o\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSTcpj-HVPnp",
        "outputId": "e6a2cb57-b2a3-47ef-d608-906bb0be74cb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abc: None\n",
            "abd: <re.Match object; span=(2, 3), match='d'>\n",
            "\n",
            "a: None\n",
            "y: None\n",
            "#: <re.Match object; span=(0, 1), match='#'>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 특정 문자열 중 하나 \n",
        "r_10 = re.compile(\"a|bc\") # \"a\" or \"bc\"\n",
        "print(\"ab:\", r_10.search(\"ab\")) # a 또는 bc 중 a 존재 => o\n",
        "print(\"bcd:\", r_10.search(\"bcd\")) # a 또는 bc 중 bc 존재 => o\n",
        "print(\"cc:\", r_10.search(\"cc\")) # a 또는 bc 중 존재x => x\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSkJF0QjVR0F",
        "outputId": "e53572aa-a976-4c12-8081-9df64f618354"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ab: <re.Match object; span=(0, 1), match='a'>\n",
            "bcd: <re.Match object; span=(0, 2), match='bc'>\n",
            "cc: None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정규표현식의 함수들"
      ],
      "metadata": {
        "id": "pO6lMU32Y7LW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab\")\n",
        "\n",
        "text1 = \"abaab abb acb abab\"\n",
        "text2 = \"babaab abb acb abab\"\n",
        "\n",
        "print(\"text1:\", text1)\n",
        "print(\"text2:\", text2)\n",
        "print()\n",
        "\n",
        "# search()\n",
        "# 문자열 전체에 대해서 정규표현시기과 매치되는지 검색\n",
        "print(\"search(text1):\", r.search(text1))\n",
        "print(\"search(text2):\", r.search(text2))\n",
        "print()\n",
        "\n",
        "# march()\n",
        "# 문자열의 처음이 정규표현식과 매치되는지 검색\n",
        "print(\"match(text1):\", r.match(text1))\n",
        "print(\"match(text2):\", r.match(text2))\n",
        "print()\n",
        "\n",
        "# split()\n",
        "# 정규표현식을 기준으로 문자열 분리 후 리스트로 반환\n",
        "print(\"split(text1):\", re.split(\" \", text1)) # (정규표현식 규칙, 분리할 문자열)\n",
        "print()\n",
        "\n",
        "\n",
        "# findall()\n",
        "# 문자열에서 정규표현식과 매치되는 모든 문자열을 찾아서 리스트로 반환\n",
        "print(\"findall(text1):\", re.findall(r, text1))\n",
        "print(\"findall(text2):\", re.findall(r, text2))\n",
        "print()\n",
        "\n",
        "# sub()\n",
        "# 문자열에서 정규표현식과 일치하는 부분을 다른 문자열로 대체\n",
        "print(\"sub(text1):\", re.sub(r, \"z\", text1)) # (정규표현식 규칙, 대체할 문자열, 대체될 문자열)\n",
        "\n",
        "# 자주 쓰이는 패턴\n",
        "# \\ : \"\\\" 문자\n",
        "# \\d : 모든 숫자, [0~9]와 동일\n",
        "# \\D : 숫자를 제외한 모든 숫자, [^0-9]와 동일\n",
        "# \\s : 공백, [\\t\\n\\r\\f\\v]와 동일\n",
        "# \\S : 공백을 제외한 문자, [^\\t\\n\\r\\f\\v]와 동일\n",
        "# \\w : 문자 또는 숫자, [^a-zA-Z0-9]와 동일\n",
        "# \\W : 문자 또는 숫자가 아닌 문자, [^a-zA-Z0-9]와 동일\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "METFqqzfVRxw",
        "outputId": "d12c9b4a-448e-49d3-9076-01ed34653a94"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: abaab abb acb abab\n",
            "text: babaab abb acb abab\n",
            "\n",
            "search(text1): <re.Match object; span=(0, 2), match='ab'>\n",
            "search(text2): <re.Match object; span=(1, 3), match='ab'>\n",
            "\n",
            "match(text1): <re.Match object; span=(0, 2), match='ab'>\n",
            "match(text2): None\n",
            "\n",
            "split(text1): ['abaab', 'abb', 'acb', 'abab']\n",
            "\n",
            "findall(text1): ['ab', 'ab', 'ab', 'ab', 'ab']\n",
            "findall(text2): ['ab', 'ab', 'ab', 'ab', 'ab']\n",
            "\n",
            "sub(text1): zaz zb acb zz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats.morestats import r_\n",
        "import re\n",
        "import time\n",
        "\n",
        "# wikepedia \"Python\"의 첫 문단\n",
        "wiki_text = 'Python is a high-level, interpreted, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.[32] Python is dykw'\n",
        "n_copy = 1000 # 처리해야할 text의 규모가 커질수록, 규칙이 복잡할수록 정규식이 더 빠름\n",
        "wiki_text = \" \".join([wiki_text]*n_copy) # n_copy 만큼 문단 복사\n",
        "\n",
        "# 대문자로 시작하고 5글자 이하인 단어 찾기 (단, ㅣ로 시작하는 단어 제외)\n",
        "# 1. 대문자로 시작\n",
        "# 2. 5글자 이하\n",
        "# 3. ㅣ로 시작 x\n",
        "\n",
        "# 정규식\n",
        "re_start = time.time()\n",
        "\n",
        "r_text = re.compile(r\"\\b[A-HJ-Z]\\w{,4}\\b\") #\\b는 단어의 경계를 의미, \\w는 문자 또는 숫자 의미\n",
        "re_list = re.findall(r_text, wiki_text)\n",
        "\n",
        "print(re_list)\n",
        "\n",
        "re_end = time.time()\n",
        "\n",
        "print(\"정규식: {0}s\". format(re_end - re_start))\n",
        "print()\n",
        "\n",
        "# split와 in\n",
        "split_in_start = time.time()\n",
        "\n",
        "word_list = wiki_text.split() # 문단을 단어로 분리\n",
        "upper_list = list() # 대문자로 시작하는 단어를 저장할 리스트\n",
        "for word_i in word_list:\n",
        "  if word_i[0].isupper() and word_i[0]!='l' and len(word_i)<=5: # 해당 단어의 첫 글자가 대문자인 경우\n",
        "    upper_list.append(word_i) #단어를 리스트에 저장\n",
        "\n",
        "print(upper_list)\n",
        "\n",
        "split_in_end = time.time()\n",
        "\n",
        "print(\"split+in: {0}s\".format(split_in_end - split_in_start))\n",
        "\n",
        "# 정규표현식을 사용하면 매우큰 텍스트도 쉽게 전처리가 가능하다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkBhEjvhVRvJ",
        "outputId": "8cdeb2a3-6640-4c53-b13e-cd684a325eb1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "정규식: 0.009790182113647461s\n",
            "\n",
            "['Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its', 'Its']\n",
            "split+in: 0.011026144027709961s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bi17OrcvVQm8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}